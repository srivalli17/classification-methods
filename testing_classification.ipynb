{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An analysis of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for the study encompasses a study of proteins that have differential expression in a learning experiment. The control used in this experiment are a set of wild type mice(functionally normal) and are compared to trisomic mice (Down's syndrome, that inherently have learning disabilities). Within the control experiment, it is further divided into 2 levels which was used to compared the learning capability with ot without exposing to Context Shock prior to the experiment. In each of those subcategories these mice were thery were subjected to saline(control) or memantine(a drug that rescues certain learning and motor capabilities under neurological condition such as Down's syndrome). Similar sets of classes were created for mice with Down's syndrome. Therefore this experiment had in total 8 classes. 'There were 38 control mice and 34 trisomic mice (Down syndrome), for a total of 72 mice. In the experiments, 15 measurements were registered of each protein per sample/mouse. Therefore, for control mice, there are 38x15, or 570 measurements, and for trisomic mice, there are 34x15, or 510 measurements. The dataset contains a total of 1080 measurements per protein. Each measurement can be considered as an independent sample/mouse'.[1]\n",
    "The objective of this dataset was to understand at cellular level which proteins are invoked under various stimuli. Using the class label as the target and 77 proteins as the features, the aim is to classify them into different classes and check the viability of the algorithms on test dataset. Therefore the aim og the project is to use features from the given dataset to classify the type of mice (based on the level of genotypic,behavioural and drug induced). Equal specimens from each class was used in the training dataset to evaluate the unknown from the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset was collected from UCI machine learning repository. However, the original dataset was from a research study - Self-Organizing Feature Maps Identify Proteins Critical to Learning in a Mouse Model of Down Syndrome - Clara Higuera,Katheleen J. Gardiner,Krzysztof J. Cios. Weblink - https://doi.org/10.1371/journal.pone.0129126 The article states that, 'This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. This dataset has been licensed under Creative Commons (CC) BY 4.0. The data is originally stored on data repository API site called figshare wherein the data can be freely downloadable which allows 1)Share — copy and redistribute the material in any medium or formatand 2) Adapt — remix, transform, and build upon the material for any purpose, even commercially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MouseID</th>\n",
       "      <th>DYRK1A_N</th>\n",
       "      <th>ITSN1_N</th>\n",
       "      <th>BDNF_N</th>\n",
       "      <th>NR1_N</th>\n",
       "      <th>NR2A_N</th>\n",
       "      <th>pAKT_N</th>\n",
       "      <th>pBRAF_N</th>\n",
       "      <th>pCAMKII_N</th>\n",
       "      <th>pCREB_N</th>\n",
       "      <th>pELK_N</th>\n",
       "      <th>pERK_N</th>\n",
       "      <th>pJNK_N</th>\n",
       "      <th>PKCA_N</th>\n",
       "      <th>pMEK_N</th>\n",
       "      <th>pNR1_N</th>\n",
       "      <th>pNR2A_N</th>\n",
       "      <th>pNR2B_N</th>\n",
       "      <th>pPKCAB_N</th>\n",
       "      <th>pRSK_N</th>\n",
       "      <th>AKT_N</th>\n",
       "      <th>BRAF_N</th>\n",
       "      <th>CAMKII_N</th>\n",
       "      <th>CREB_N</th>\n",
       "      <th>ELK_N</th>\n",
       "      <th>ERK_N</th>\n",
       "      <th>GSK3B_N</th>\n",
       "      <th>JNK_N</th>\n",
       "      <th>MEK_N</th>\n",
       "      <th>TRKA_N</th>\n",
       "      <th>RSK_N</th>\n",
       "      <th>APP_N</th>\n",
       "      <th>Bcatenin_N</th>\n",
       "      <th>SOD1_N</th>\n",
       "      <th>MTOR_N</th>\n",
       "      <th>P38_N</th>\n",
       "      <th>pMTOR_N</th>\n",
       "      <th>DSCR1_N</th>\n",
       "      <th>AMPKA_N</th>\n",
       "      <th>NR2B_N</th>\n",
       "      <th>pNUMB_N</th>\n",
       "      <th>RAPTOR_N</th>\n",
       "      <th>TIAM1_N</th>\n",
       "      <th>pP70S6_N</th>\n",
       "      <th>NUMB_N</th>\n",
       "      <th>P70S6_N</th>\n",
       "      <th>pGSK3B_N</th>\n",
       "      <th>pPKCG_N</th>\n",
       "      <th>CDK5_N</th>\n",
       "      <th>S6_N</th>\n",
       "      <th>ADARB1_N</th>\n",
       "      <th>AcetylH3K9_N</th>\n",
       "      <th>RRP1_N</th>\n",
       "      <th>BAX_N</th>\n",
       "      <th>ARC_N</th>\n",
       "      <th>ERBB4_N</th>\n",
       "      <th>nNOS_N</th>\n",
       "      <th>Tau_N</th>\n",
       "      <th>GFAP_N</th>\n",
       "      <th>GluR3_N</th>\n",
       "      <th>GluR4_N</th>\n",
       "      <th>IL1B_N</th>\n",
       "      <th>P3525_N</th>\n",
       "      <th>pCASP9_N</th>\n",
       "      <th>PSD95_N</th>\n",
       "      <th>SNCA_N</th>\n",
       "      <th>Ubiquitin_N</th>\n",
       "      <th>pGSK3B_Tyr216_N</th>\n",
       "      <th>SHH_N</th>\n",
       "      <th>BAD_N</th>\n",
       "      <th>BCL2_N</th>\n",
       "      <th>pS6_N</th>\n",
       "      <th>pCFOS_N</th>\n",
       "      <th>SYP_N</th>\n",
       "      <th>H3AcK18_N</th>\n",
       "      <th>EGR1_N</th>\n",
       "      <th>H3MeK4_N</th>\n",
       "      <th>CaNA_N</th>\n",
       "      <th>Genotype</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>Behavior</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>309_1</td>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.747193</td>\n",
       "      <td>0.430175</td>\n",
       "      <td>2.816329</td>\n",
       "      <td>5.990152</td>\n",
       "      <td>0.218830</td>\n",
       "      <td>0.177565</td>\n",
       "      <td>2.373744</td>\n",
       "      <td>0.232224</td>\n",
       "      <td>1.750936</td>\n",
       "      <td>0.687906</td>\n",
       "      <td>0.306382</td>\n",
       "      <td>0.402698</td>\n",
       "      <td>0.296927</td>\n",
       "      <td>1.022060</td>\n",
       "      <td>0.605673</td>\n",
       "      <td>1.877684</td>\n",
       "      <td>2.308745</td>\n",
       "      <td>0.441599</td>\n",
       "      <td>0.859366</td>\n",
       "      <td>0.416289</td>\n",
       "      <td>0.369608</td>\n",
       "      <td>0.178944</td>\n",
       "      <td>1.866358</td>\n",
       "      <td>3.685247</td>\n",
       "      <td>1.537227</td>\n",
       "      <td>0.264526</td>\n",
       "      <td>0.319677</td>\n",
       "      <td>0.813866</td>\n",
       "      <td>0.165846</td>\n",
       "      <td>0.453910</td>\n",
       "      <td>3.037621</td>\n",
       "      <td>0.369510</td>\n",
       "      <td>0.458539</td>\n",
       "      <td>0.335336</td>\n",
       "      <td>0.825192</td>\n",
       "      <td>0.576916</td>\n",
       "      <td>0.448099</td>\n",
       "      <td>0.586271</td>\n",
       "      <td>0.394721</td>\n",
       "      <td>0.339571</td>\n",
       "      <td>0.482864</td>\n",
       "      <td>0.294170</td>\n",
       "      <td>0.182150</td>\n",
       "      <td>0.842725</td>\n",
       "      <td>0.192608</td>\n",
       "      <td>1.443091</td>\n",
       "      <td>0.294700</td>\n",
       "      <td>0.354605</td>\n",
       "      <td>1.339070</td>\n",
       "      <td>0.170119</td>\n",
       "      <td>0.159102</td>\n",
       "      <td>0.188852</td>\n",
       "      <td>0.106305</td>\n",
       "      <td>0.144989</td>\n",
       "      <td>0.176668</td>\n",
       "      <td>0.125190</td>\n",
       "      <td>0.115291</td>\n",
       "      <td>0.228043</td>\n",
       "      <td>0.142756</td>\n",
       "      <td>0.430957</td>\n",
       "      <td>0.247538</td>\n",
       "      <td>1.603310</td>\n",
       "      <td>2.014875</td>\n",
       "      <td>0.108234</td>\n",
       "      <td>1.044979</td>\n",
       "      <td>0.831557</td>\n",
       "      <td>0.188852</td>\n",
       "      <td>0.122652</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.106305</td>\n",
       "      <td>0.108336</td>\n",
       "      <td>0.427099</td>\n",
       "      <td>0.114783</td>\n",
       "      <td>0.131790</td>\n",
       "      <td>0.128186</td>\n",
       "      <td>1.675652</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>309_2</td>\n",
       "      <td>0.514617</td>\n",
       "      <td>0.689064</td>\n",
       "      <td>0.411770</td>\n",
       "      <td>2.789514</td>\n",
       "      <td>5.685038</td>\n",
       "      <td>0.211636</td>\n",
       "      <td>0.172817</td>\n",
       "      <td>2.292150</td>\n",
       "      <td>0.226972</td>\n",
       "      <td>1.596377</td>\n",
       "      <td>0.695006</td>\n",
       "      <td>0.299051</td>\n",
       "      <td>0.385987</td>\n",
       "      <td>0.281319</td>\n",
       "      <td>0.956676</td>\n",
       "      <td>0.587559</td>\n",
       "      <td>1.725774</td>\n",
       "      <td>2.043037</td>\n",
       "      <td>0.445222</td>\n",
       "      <td>0.834659</td>\n",
       "      <td>0.400364</td>\n",
       "      <td>0.356178</td>\n",
       "      <td>0.173680</td>\n",
       "      <td>1.761047</td>\n",
       "      <td>3.485287</td>\n",
       "      <td>1.509249</td>\n",
       "      <td>0.255727</td>\n",
       "      <td>0.304419</td>\n",
       "      <td>0.780504</td>\n",
       "      <td>0.157194</td>\n",
       "      <td>0.430940</td>\n",
       "      <td>2.921882</td>\n",
       "      <td>0.342279</td>\n",
       "      <td>0.423560</td>\n",
       "      <td>0.324835</td>\n",
       "      <td>0.761718</td>\n",
       "      <td>0.545097</td>\n",
       "      <td>0.420876</td>\n",
       "      <td>0.545097</td>\n",
       "      <td>0.368255</td>\n",
       "      <td>0.321959</td>\n",
       "      <td>0.454519</td>\n",
       "      <td>0.276431</td>\n",
       "      <td>0.182086</td>\n",
       "      <td>0.847615</td>\n",
       "      <td>0.194815</td>\n",
       "      <td>1.439460</td>\n",
       "      <td>0.294060</td>\n",
       "      <td>0.354548</td>\n",
       "      <td>1.306323</td>\n",
       "      <td>0.171427</td>\n",
       "      <td>0.158129</td>\n",
       "      <td>0.184570</td>\n",
       "      <td>0.106592</td>\n",
       "      <td>0.150471</td>\n",
       "      <td>0.178309</td>\n",
       "      <td>0.134275</td>\n",
       "      <td>0.118235</td>\n",
       "      <td>0.238073</td>\n",
       "      <td>0.142037</td>\n",
       "      <td>0.457156</td>\n",
       "      <td>0.257632</td>\n",
       "      <td>1.671738</td>\n",
       "      <td>2.004605</td>\n",
       "      <td>0.109749</td>\n",
       "      <td>1.009883</td>\n",
       "      <td>0.849270</td>\n",
       "      <td>0.200404</td>\n",
       "      <td>0.116682</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.106592</td>\n",
       "      <td>0.104315</td>\n",
       "      <td>0.441581</td>\n",
       "      <td>0.111974</td>\n",
       "      <td>0.135103</td>\n",
       "      <td>0.131119</td>\n",
       "      <td>1.743610</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>309_3</td>\n",
       "      <td>0.509183</td>\n",
       "      <td>0.730247</td>\n",
       "      <td>0.418309</td>\n",
       "      <td>2.687201</td>\n",
       "      <td>5.622059</td>\n",
       "      <td>0.209011</td>\n",
       "      <td>0.175722</td>\n",
       "      <td>2.283337</td>\n",
       "      <td>0.230247</td>\n",
       "      <td>1.561316</td>\n",
       "      <td>0.677348</td>\n",
       "      <td>0.291276</td>\n",
       "      <td>0.381002</td>\n",
       "      <td>0.281710</td>\n",
       "      <td>1.003635</td>\n",
       "      <td>0.602449</td>\n",
       "      <td>1.731873</td>\n",
       "      <td>2.017984</td>\n",
       "      <td>0.467668</td>\n",
       "      <td>0.814329</td>\n",
       "      <td>0.399847</td>\n",
       "      <td>0.368089</td>\n",
       "      <td>0.173905</td>\n",
       "      <td>1.765544</td>\n",
       "      <td>3.571456</td>\n",
       "      <td>1.501244</td>\n",
       "      <td>0.259614</td>\n",
       "      <td>0.311747</td>\n",
       "      <td>0.785154</td>\n",
       "      <td>0.160895</td>\n",
       "      <td>0.423187</td>\n",
       "      <td>2.944136</td>\n",
       "      <td>0.343696</td>\n",
       "      <td>0.425005</td>\n",
       "      <td>0.324852</td>\n",
       "      <td>0.757031</td>\n",
       "      <td>0.543620</td>\n",
       "      <td>0.404630</td>\n",
       "      <td>0.552994</td>\n",
       "      <td>0.363880</td>\n",
       "      <td>0.313086</td>\n",
       "      <td>0.447197</td>\n",
       "      <td>0.256648</td>\n",
       "      <td>0.184388</td>\n",
       "      <td>0.856166</td>\n",
       "      <td>0.200737</td>\n",
       "      <td>1.524364</td>\n",
       "      <td>0.301881</td>\n",
       "      <td>0.386087</td>\n",
       "      <td>1.279600</td>\n",
       "      <td>0.185456</td>\n",
       "      <td>0.148696</td>\n",
       "      <td>0.190532</td>\n",
       "      <td>0.108303</td>\n",
       "      <td>0.145330</td>\n",
       "      <td>0.176213</td>\n",
       "      <td>0.132560</td>\n",
       "      <td>0.117760</td>\n",
       "      <td>0.244817</td>\n",
       "      <td>0.142445</td>\n",
       "      <td>0.510472</td>\n",
       "      <td>0.255343</td>\n",
       "      <td>1.663550</td>\n",
       "      <td>2.016831</td>\n",
       "      <td>0.108196</td>\n",
       "      <td>0.996848</td>\n",
       "      <td>0.846709</td>\n",
       "      <td>0.193685</td>\n",
       "      <td>0.118508</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.108303</td>\n",
       "      <td>0.106219</td>\n",
       "      <td>0.435777</td>\n",
       "      <td>0.111883</td>\n",
       "      <td>0.133362</td>\n",
       "      <td>0.127431</td>\n",
       "      <td>1.926427</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>309_4</td>\n",
       "      <td>0.442107</td>\n",
       "      <td>0.617076</td>\n",
       "      <td>0.358626</td>\n",
       "      <td>2.466947</td>\n",
       "      <td>4.979503</td>\n",
       "      <td>0.222886</td>\n",
       "      <td>0.176463</td>\n",
       "      <td>2.152301</td>\n",
       "      <td>0.207004</td>\n",
       "      <td>1.595086</td>\n",
       "      <td>0.583277</td>\n",
       "      <td>0.296729</td>\n",
       "      <td>0.377087</td>\n",
       "      <td>0.313832</td>\n",
       "      <td>0.875390</td>\n",
       "      <td>0.520293</td>\n",
       "      <td>1.566852</td>\n",
       "      <td>2.132754</td>\n",
       "      <td>0.477671</td>\n",
       "      <td>0.727705</td>\n",
       "      <td>0.385639</td>\n",
       "      <td>0.362970</td>\n",
       "      <td>0.179449</td>\n",
       "      <td>1.286277</td>\n",
       "      <td>2.970137</td>\n",
       "      <td>1.419710</td>\n",
       "      <td>0.259536</td>\n",
       "      <td>0.279218</td>\n",
       "      <td>0.734492</td>\n",
       "      <td>0.162210</td>\n",
       "      <td>0.410615</td>\n",
       "      <td>2.500204</td>\n",
       "      <td>0.344509</td>\n",
       "      <td>0.429211</td>\n",
       "      <td>0.330121</td>\n",
       "      <td>0.746980</td>\n",
       "      <td>0.546763</td>\n",
       "      <td>0.386860</td>\n",
       "      <td>0.547849</td>\n",
       "      <td>0.366771</td>\n",
       "      <td>0.328492</td>\n",
       "      <td>0.442650</td>\n",
       "      <td>0.398534</td>\n",
       "      <td>0.161768</td>\n",
       "      <td>0.760234</td>\n",
       "      <td>0.184169</td>\n",
       "      <td>1.612382</td>\n",
       "      <td>0.296382</td>\n",
       "      <td>0.290680</td>\n",
       "      <td>1.198765</td>\n",
       "      <td>0.159799</td>\n",
       "      <td>0.166112</td>\n",
       "      <td>0.185323</td>\n",
       "      <td>0.103184</td>\n",
       "      <td>0.140656</td>\n",
       "      <td>0.163804</td>\n",
       "      <td>0.123210</td>\n",
       "      <td>0.117439</td>\n",
       "      <td>0.234947</td>\n",
       "      <td>0.145068</td>\n",
       "      <td>0.430996</td>\n",
       "      <td>0.251103</td>\n",
       "      <td>1.484624</td>\n",
       "      <td>1.957233</td>\n",
       "      <td>0.119883</td>\n",
       "      <td>0.990225</td>\n",
       "      <td>0.833277</td>\n",
       "      <td>0.192112</td>\n",
       "      <td>0.132781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.103184</td>\n",
       "      <td>0.111262</td>\n",
       "      <td>0.391691</td>\n",
       "      <td>0.130405</td>\n",
       "      <td>0.147444</td>\n",
       "      <td>0.146901</td>\n",
       "      <td>1.700563</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>309_5</td>\n",
       "      <td>0.434940</td>\n",
       "      <td>0.617430</td>\n",
       "      <td>0.358802</td>\n",
       "      <td>2.365785</td>\n",
       "      <td>4.718679</td>\n",
       "      <td>0.213106</td>\n",
       "      <td>0.173627</td>\n",
       "      <td>2.134014</td>\n",
       "      <td>0.192158</td>\n",
       "      <td>1.504230</td>\n",
       "      <td>0.550960</td>\n",
       "      <td>0.286961</td>\n",
       "      <td>0.363502</td>\n",
       "      <td>0.277964</td>\n",
       "      <td>0.864912</td>\n",
       "      <td>0.507990</td>\n",
       "      <td>1.480059</td>\n",
       "      <td>2.013697</td>\n",
       "      <td>0.483416</td>\n",
       "      <td>0.687794</td>\n",
       "      <td>0.367531</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>0.174836</td>\n",
       "      <td>1.324695</td>\n",
       "      <td>2.896334</td>\n",
       "      <td>1.359876</td>\n",
       "      <td>0.250705</td>\n",
       "      <td>0.273667</td>\n",
       "      <td>0.702699</td>\n",
       "      <td>0.154827</td>\n",
       "      <td>0.398550</td>\n",
       "      <td>2.456560</td>\n",
       "      <td>0.329126</td>\n",
       "      <td>0.408755</td>\n",
       "      <td>0.313415</td>\n",
       "      <td>0.691956</td>\n",
       "      <td>0.536860</td>\n",
       "      <td>0.360816</td>\n",
       "      <td>0.512824</td>\n",
       "      <td>0.351551</td>\n",
       "      <td>0.312206</td>\n",
       "      <td>0.419095</td>\n",
       "      <td>0.393447</td>\n",
       "      <td>0.160200</td>\n",
       "      <td>0.768113</td>\n",
       "      <td>0.185718</td>\n",
       "      <td>1.645807</td>\n",
       "      <td>0.296829</td>\n",
       "      <td>0.309345</td>\n",
       "      <td>1.206995</td>\n",
       "      <td>0.164650</td>\n",
       "      <td>0.160687</td>\n",
       "      <td>0.188221</td>\n",
       "      <td>0.104784</td>\n",
       "      <td>0.141983</td>\n",
       "      <td>0.167710</td>\n",
       "      <td>0.136838</td>\n",
       "      <td>0.116048</td>\n",
       "      <td>0.255528</td>\n",
       "      <td>0.140871</td>\n",
       "      <td>0.481227</td>\n",
       "      <td>0.251773</td>\n",
       "      <td>1.534835</td>\n",
       "      <td>2.009109</td>\n",
       "      <td>0.119524</td>\n",
       "      <td>0.997775</td>\n",
       "      <td>0.878668</td>\n",
       "      <td>0.205604</td>\n",
       "      <td>0.129954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.104784</td>\n",
       "      <td>0.110694</td>\n",
       "      <td>0.434154</td>\n",
       "      <td>0.118481</td>\n",
       "      <td>0.140314</td>\n",
       "      <td>0.148380</td>\n",
       "      <td>1.839730</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  MouseID  DYRK1A_N   ITSN1_N    BDNF_N     NR1_N    NR2A_N    pAKT_N  \\\n",
       "0   309_1  0.503644  0.747193  0.430175  2.816329  5.990152  0.218830   \n",
       "1   309_2  0.514617  0.689064  0.411770  2.789514  5.685038  0.211636   \n",
       "2   309_3  0.509183  0.730247  0.418309  2.687201  5.622059  0.209011   \n",
       "3   309_4  0.442107  0.617076  0.358626  2.466947  4.979503  0.222886   \n",
       "4   309_5  0.434940  0.617430  0.358802  2.365785  4.718679  0.213106   \n",
       "\n",
       "    pBRAF_N  pCAMKII_N   pCREB_N    pELK_N    pERK_N    pJNK_N    PKCA_N  \\\n",
       "0  0.177565   2.373744  0.232224  1.750936  0.687906  0.306382  0.402698   \n",
       "1  0.172817   2.292150  0.226972  1.596377  0.695006  0.299051  0.385987   \n",
       "2  0.175722   2.283337  0.230247  1.561316  0.677348  0.291276  0.381002   \n",
       "3  0.176463   2.152301  0.207004  1.595086  0.583277  0.296729  0.377087   \n",
       "4  0.173627   2.134014  0.192158  1.504230  0.550960  0.286961  0.363502   \n",
       "\n",
       "     pMEK_N    pNR1_N   pNR2A_N   pNR2B_N  pPKCAB_N    pRSK_N     AKT_N  \\\n",
       "0  0.296927  1.022060  0.605673  1.877684  2.308745  0.441599  0.859366   \n",
       "1  0.281319  0.956676  0.587559  1.725774  2.043037  0.445222  0.834659   \n",
       "2  0.281710  1.003635  0.602449  1.731873  2.017984  0.467668  0.814329   \n",
       "3  0.313832  0.875390  0.520293  1.566852  2.132754  0.477671  0.727705   \n",
       "4  0.277964  0.864912  0.507990  1.480059  2.013697  0.483416  0.687794   \n",
       "\n",
       "     BRAF_N  CAMKII_N    CREB_N     ELK_N     ERK_N   GSK3B_N     JNK_N  \\\n",
       "0  0.416289  0.369608  0.178944  1.866358  3.685247  1.537227  0.264526   \n",
       "1  0.400364  0.356178  0.173680  1.761047  3.485287  1.509249  0.255727   \n",
       "2  0.399847  0.368089  0.173905  1.765544  3.571456  1.501244  0.259614   \n",
       "3  0.385639  0.362970  0.179449  1.286277  2.970137  1.419710  0.259536   \n",
       "4  0.367531  0.355311  0.174836  1.324695  2.896334  1.359876  0.250705   \n",
       "\n",
       "      MEK_N    TRKA_N     RSK_N     APP_N  Bcatenin_N    SOD1_N    MTOR_N  \\\n",
       "0  0.319677  0.813866  0.165846  0.453910    3.037621  0.369510  0.458539   \n",
       "1  0.304419  0.780504  0.157194  0.430940    2.921882  0.342279  0.423560   \n",
       "2  0.311747  0.785154  0.160895  0.423187    2.944136  0.343696  0.425005   \n",
       "3  0.279218  0.734492  0.162210  0.410615    2.500204  0.344509  0.429211   \n",
       "4  0.273667  0.702699  0.154827  0.398550    2.456560  0.329126  0.408755   \n",
       "\n",
       "      P38_N   pMTOR_N   DSCR1_N   AMPKA_N    NR2B_N   pNUMB_N  RAPTOR_N  \\\n",
       "0  0.335336  0.825192  0.576916  0.448099  0.586271  0.394721  0.339571   \n",
       "1  0.324835  0.761718  0.545097  0.420876  0.545097  0.368255  0.321959   \n",
       "2  0.324852  0.757031  0.543620  0.404630  0.552994  0.363880  0.313086   \n",
       "3  0.330121  0.746980  0.546763  0.386860  0.547849  0.366771  0.328492   \n",
       "4  0.313415  0.691956  0.536860  0.360816  0.512824  0.351551  0.312206   \n",
       "\n",
       "    TIAM1_N  pP70S6_N    NUMB_N   P70S6_N  pGSK3B_N   pPKCG_N    CDK5_N  \\\n",
       "0  0.482864  0.294170  0.182150  0.842725  0.192608  1.443091  0.294700   \n",
       "1  0.454519  0.276431  0.182086  0.847615  0.194815  1.439460  0.294060   \n",
       "2  0.447197  0.256648  0.184388  0.856166  0.200737  1.524364  0.301881   \n",
       "3  0.442650  0.398534  0.161768  0.760234  0.184169  1.612382  0.296382   \n",
       "4  0.419095  0.393447  0.160200  0.768113  0.185718  1.645807  0.296829   \n",
       "\n",
       "       S6_N  ADARB1_N  AcetylH3K9_N    RRP1_N     BAX_N     ARC_N   ERBB4_N  \\\n",
       "0  0.354605  1.339070      0.170119  0.159102  0.188852  0.106305  0.144989   \n",
       "1  0.354548  1.306323      0.171427  0.158129  0.184570  0.106592  0.150471   \n",
       "2  0.386087  1.279600      0.185456  0.148696  0.190532  0.108303  0.145330   \n",
       "3  0.290680  1.198765      0.159799  0.166112  0.185323  0.103184  0.140656   \n",
       "4  0.309345  1.206995      0.164650  0.160687  0.188221  0.104784  0.141983   \n",
       "\n",
       "     nNOS_N     Tau_N    GFAP_N   GluR3_N   GluR4_N    IL1B_N   P3525_N  \\\n",
       "0  0.176668  0.125190  0.115291  0.228043  0.142756  0.430957  0.247538   \n",
       "1  0.178309  0.134275  0.118235  0.238073  0.142037  0.457156  0.257632   \n",
       "2  0.176213  0.132560  0.117760  0.244817  0.142445  0.510472  0.255343   \n",
       "3  0.163804  0.123210  0.117439  0.234947  0.145068  0.430996  0.251103   \n",
       "4  0.167710  0.136838  0.116048  0.255528  0.140871  0.481227  0.251773   \n",
       "\n",
       "   pCASP9_N   PSD95_N    SNCA_N  Ubiquitin_N  pGSK3B_Tyr216_N     SHH_N  \\\n",
       "0  1.603310  2.014875  0.108234     1.044979         0.831557  0.188852   \n",
       "1  1.671738  2.004605  0.109749     1.009883         0.849270  0.200404   \n",
       "2  1.663550  2.016831  0.108196     0.996848         0.846709  0.193685   \n",
       "3  1.484624  1.957233  0.119883     0.990225         0.833277  0.192112   \n",
       "4  1.534835  2.009109  0.119524     0.997775         0.878668  0.205604   \n",
       "\n",
       "      BAD_N  BCL2_N     pS6_N   pCFOS_N     SYP_N  H3AcK18_N    EGR1_N  \\\n",
       "0  0.122652     NaN  0.106305  0.108336  0.427099   0.114783  0.131790   \n",
       "1  0.116682     NaN  0.106592  0.104315  0.441581   0.111974  0.135103   \n",
       "2  0.118508     NaN  0.108303  0.106219  0.435777   0.111883  0.133362   \n",
       "3  0.132781     NaN  0.103184  0.111262  0.391691   0.130405  0.147444   \n",
       "4  0.129954     NaN  0.104784  0.110694  0.434154   0.118481  0.140314   \n",
       "\n",
       "   H3MeK4_N    CaNA_N Genotype  Treatment Behavior   class  \n",
       "0  0.128186  1.675652  Control  Memantine      C/S  c-CS-m  \n",
       "1  0.131119  1.743610  Control  Memantine      C/S  c-CS-m  \n",
       "2  0.127431  1.926427  Control  Memantine      C/S  c-CS-m  \n",
       "3  0.146901  1.700563  Control  Memantine      C/S  c-CS-m  \n",
       "4  0.148380  1.839730  Control  Memantine      C/S  c-CS-m  "
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "link = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls'\n",
    "print('URL:', link)\n",
    "\n",
    "df = pd.read_excel(link,\n",
    "                 encoding='utf-8')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target column was not preprocessed into a pipeline because in predict function, its requires only the features as an input to predict the traget as the output. Therefore the class column was converted into an integer class from mapping function before implementing all the preprocesses applied to the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c-CS-m': 0,\n",
       " 'c-CS-s': 1,\n",
       " 'c-SC-m': 2,\n",
       " 'c-SC-s': 3,\n",
       " 't-CS-m': 4,\n",
       " 't-CS-s': 5,\n",
       " 't-SC-m': 6,\n",
       " 't-SC-s': 7}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(df['class']))}\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MouseID</th>\n",
       "      <th>DYRK1A_N</th>\n",
       "      <th>ITSN1_N</th>\n",
       "      <th>BDNF_N</th>\n",
       "      <th>NR1_N</th>\n",
       "      <th>NR2A_N</th>\n",
       "      <th>pAKT_N</th>\n",
       "      <th>pBRAF_N</th>\n",
       "      <th>pCAMKII_N</th>\n",
       "      <th>pCREB_N</th>\n",
       "      <th>pELK_N</th>\n",
       "      <th>pERK_N</th>\n",
       "      <th>pJNK_N</th>\n",
       "      <th>PKCA_N</th>\n",
       "      <th>pMEK_N</th>\n",
       "      <th>pNR1_N</th>\n",
       "      <th>pNR2A_N</th>\n",
       "      <th>pNR2B_N</th>\n",
       "      <th>pPKCAB_N</th>\n",
       "      <th>pRSK_N</th>\n",
       "      <th>AKT_N</th>\n",
       "      <th>BRAF_N</th>\n",
       "      <th>CAMKII_N</th>\n",
       "      <th>CREB_N</th>\n",
       "      <th>ELK_N</th>\n",
       "      <th>ERK_N</th>\n",
       "      <th>GSK3B_N</th>\n",
       "      <th>JNK_N</th>\n",
       "      <th>MEK_N</th>\n",
       "      <th>TRKA_N</th>\n",
       "      <th>RSK_N</th>\n",
       "      <th>APP_N</th>\n",
       "      <th>Bcatenin_N</th>\n",
       "      <th>SOD1_N</th>\n",
       "      <th>MTOR_N</th>\n",
       "      <th>P38_N</th>\n",
       "      <th>pMTOR_N</th>\n",
       "      <th>DSCR1_N</th>\n",
       "      <th>AMPKA_N</th>\n",
       "      <th>NR2B_N</th>\n",
       "      <th>pNUMB_N</th>\n",
       "      <th>RAPTOR_N</th>\n",
       "      <th>TIAM1_N</th>\n",
       "      <th>pP70S6_N</th>\n",
       "      <th>NUMB_N</th>\n",
       "      <th>P70S6_N</th>\n",
       "      <th>pGSK3B_N</th>\n",
       "      <th>pPKCG_N</th>\n",
       "      <th>CDK5_N</th>\n",
       "      <th>S6_N</th>\n",
       "      <th>ADARB1_N</th>\n",
       "      <th>AcetylH3K9_N</th>\n",
       "      <th>RRP1_N</th>\n",
       "      <th>BAX_N</th>\n",
       "      <th>ARC_N</th>\n",
       "      <th>ERBB4_N</th>\n",
       "      <th>nNOS_N</th>\n",
       "      <th>Tau_N</th>\n",
       "      <th>GFAP_N</th>\n",
       "      <th>GluR3_N</th>\n",
       "      <th>GluR4_N</th>\n",
       "      <th>IL1B_N</th>\n",
       "      <th>P3525_N</th>\n",
       "      <th>pCASP9_N</th>\n",
       "      <th>PSD95_N</th>\n",
       "      <th>SNCA_N</th>\n",
       "      <th>Ubiquitin_N</th>\n",
       "      <th>pGSK3B_Tyr216_N</th>\n",
       "      <th>SHH_N</th>\n",
       "      <th>BAD_N</th>\n",
       "      <th>BCL2_N</th>\n",
       "      <th>pS6_N</th>\n",
       "      <th>pCFOS_N</th>\n",
       "      <th>SYP_N</th>\n",
       "      <th>H3AcK18_N</th>\n",
       "      <th>EGR1_N</th>\n",
       "      <th>H3MeK4_N</th>\n",
       "      <th>CaNA_N</th>\n",
       "      <th>Genotype</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>Behavior</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>309_1</td>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.747193</td>\n",
       "      <td>0.430175</td>\n",
       "      <td>2.816329</td>\n",
       "      <td>5.990152</td>\n",
       "      <td>0.218830</td>\n",
       "      <td>0.177565</td>\n",
       "      <td>2.373744</td>\n",
       "      <td>0.232224</td>\n",
       "      <td>1.750936</td>\n",
       "      <td>0.687906</td>\n",
       "      <td>0.306382</td>\n",
       "      <td>0.402698</td>\n",
       "      <td>0.296927</td>\n",
       "      <td>1.022060</td>\n",
       "      <td>0.605673</td>\n",
       "      <td>1.877684</td>\n",
       "      <td>2.308745</td>\n",
       "      <td>0.441599</td>\n",
       "      <td>0.859366</td>\n",
       "      <td>0.416289</td>\n",
       "      <td>0.369608</td>\n",
       "      <td>0.178944</td>\n",
       "      <td>1.866358</td>\n",
       "      <td>3.685247</td>\n",
       "      <td>1.537227</td>\n",
       "      <td>0.264526</td>\n",
       "      <td>0.319677</td>\n",
       "      <td>0.813866</td>\n",
       "      <td>0.165846</td>\n",
       "      <td>0.453910</td>\n",
       "      <td>3.037621</td>\n",
       "      <td>0.369510</td>\n",
       "      <td>0.458539</td>\n",
       "      <td>0.335336</td>\n",
       "      <td>0.825192</td>\n",
       "      <td>0.576916</td>\n",
       "      <td>0.448099</td>\n",
       "      <td>0.586271</td>\n",
       "      <td>0.394721</td>\n",
       "      <td>0.339571</td>\n",
       "      <td>0.482864</td>\n",
       "      <td>0.294170</td>\n",
       "      <td>0.182150</td>\n",
       "      <td>0.842725</td>\n",
       "      <td>0.192608</td>\n",
       "      <td>1.443091</td>\n",
       "      <td>0.294700</td>\n",
       "      <td>0.354605</td>\n",
       "      <td>1.339070</td>\n",
       "      <td>0.170119</td>\n",
       "      <td>0.159102</td>\n",
       "      <td>0.188852</td>\n",
       "      <td>0.106305</td>\n",
       "      <td>0.144989</td>\n",
       "      <td>0.176668</td>\n",
       "      <td>0.125190</td>\n",
       "      <td>0.115291</td>\n",
       "      <td>0.228043</td>\n",
       "      <td>0.142756</td>\n",
       "      <td>0.430957</td>\n",
       "      <td>0.247538</td>\n",
       "      <td>1.603310</td>\n",
       "      <td>2.014875</td>\n",
       "      <td>0.108234</td>\n",
       "      <td>1.044979</td>\n",
       "      <td>0.831557</td>\n",
       "      <td>0.188852</td>\n",
       "      <td>0.122652</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.106305</td>\n",
       "      <td>0.108336</td>\n",
       "      <td>0.427099</td>\n",
       "      <td>0.114783</td>\n",
       "      <td>0.131790</td>\n",
       "      <td>0.128186</td>\n",
       "      <td>1.675652</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>309_2</td>\n",
       "      <td>0.514617</td>\n",
       "      <td>0.689064</td>\n",
       "      <td>0.411770</td>\n",
       "      <td>2.789514</td>\n",
       "      <td>5.685038</td>\n",
       "      <td>0.211636</td>\n",
       "      <td>0.172817</td>\n",
       "      <td>2.292150</td>\n",
       "      <td>0.226972</td>\n",
       "      <td>1.596377</td>\n",
       "      <td>0.695006</td>\n",
       "      <td>0.299051</td>\n",
       "      <td>0.385987</td>\n",
       "      <td>0.281319</td>\n",
       "      <td>0.956676</td>\n",
       "      <td>0.587559</td>\n",
       "      <td>1.725774</td>\n",
       "      <td>2.043037</td>\n",
       "      <td>0.445222</td>\n",
       "      <td>0.834659</td>\n",
       "      <td>0.400364</td>\n",
       "      <td>0.356178</td>\n",
       "      <td>0.173680</td>\n",
       "      <td>1.761047</td>\n",
       "      <td>3.485287</td>\n",
       "      <td>1.509249</td>\n",
       "      <td>0.255727</td>\n",
       "      <td>0.304419</td>\n",
       "      <td>0.780504</td>\n",
       "      <td>0.157194</td>\n",
       "      <td>0.430940</td>\n",
       "      <td>2.921882</td>\n",
       "      <td>0.342279</td>\n",
       "      <td>0.423560</td>\n",
       "      <td>0.324835</td>\n",
       "      <td>0.761718</td>\n",
       "      <td>0.545097</td>\n",
       "      <td>0.420876</td>\n",
       "      <td>0.545097</td>\n",
       "      <td>0.368255</td>\n",
       "      <td>0.321959</td>\n",
       "      <td>0.454519</td>\n",
       "      <td>0.276431</td>\n",
       "      <td>0.182086</td>\n",
       "      <td>0.847615</td>\n",
       "      <td>0.194815</td>\n",
       "      <td>1.439460</td>\n",
       "      <td>0.294060</td>\n",
       "      <td>0.354548</td>\n",
       "      <td>1.306323</td>\n",
       "      <td>0.171427</td>\n",
       "      <td>0.158129</td>\n",
       "      <td>0.184570</td>\n",
       "      <td>0.106592</td>\n",
       "      <td>0.150471</td>\n",
       "      <td>0.178309</td>\n",
       "      <td>0.134275</td>\n",
       "      <td>0.118235</td>\n",
       "      <td>0.238073</td>\n",
       "      <td>0.142037</td>\n",
       "      <td>0.457156</td>\n",
       "      <td>0.257632</td>\n",
       "      <td>1.671738</td>\n",
       "      <td>2.004605</td>\n",
       "      <td>0.109749</td>\n",
       "      <td>1.009883</td>\n",
       "      <td>0.849270</td>\n",
       "      <td>0.200404</td>\n",
       "      <td>0.116682</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.106592</td>\n",
       "      <td>0.104315</td>\n",
       "      <td>0.441581</td>\n",
       "      <td>0.111974</td>\n",
       "      <td>0.135103</td>\n",
       "      <td>0.131119</td>\n",
       "      <td>1.743610</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>309_3</td>\n",
       "      <td>0.509183</td>\n",
       "      <td>0.730247</td>\n",
       "      <td>0.418309</td>\n",
       "      <td>2.687201</td>\n",
       "      <td>5.622059</td>\n",
       "      <td>0.209011</td>\n",
       "      <td>0.175722</td>\n",
       "      <td>2.283337</td>\n",
       "      <td>0.230247</td>\n",
       "      <td>1.561316</td>\n",
       "      <td>0.677348</td>\n",
       "      <td>0.291276</td>\n",
       "      <td>0.381002</td>\n",
       "      <td>0.281710</td>\n",
       "      <td>1.003635</td>\n",
       "      <td>0.602449</td>\n",
       "      <td>1.731873</td>\n",
       "      <td>2.017984</td>\n",
       "      <td>0.467668</td>\n",
       "      <td>0.814329</td>\n",
       "      <td>0.399847</td>\n",
       "      <td>0.368089</td>\n",
       "      <td>0.173905</td>\n",
       "      <td>1.765544</td>\n",
       "      <td>3.571456</td>\n",
       "      <td>1.501244</td>\n",
       "      <td>0.259614</td>\n",
       "      <td>0.311747</td>\n",
       "      <td>0.785154</td>\n",
       "      <td>0.160895</td>\n",
       "      <td>0.423187</td>\n",
       "      <td>2.944136</td>\n",
       "      <td>0.343696</td>\n",
       "      <td>0.425005</td>\n",
       "      <td>0.324852</td>\n",
       "      <td>0.757031</td>\n",
       "      <td>0.543620</td>\n",
       "      <td>0.404630</td>\n",
       "      <td>0.552994</td>\n",
       "      <td>0.363880</td>\n",
       "      <td>0.313086</td>\n",
       "      <td>0.447197</td>\n",
       "      <td>0.256648</td>\n",
       "      <td>0.184388</td>\n",
       "      <td>0.856166</td>\n",
       "      <td>0.200737</td>\n",
       "      <td>1.524364</td>\n",
       "      <td>0.301881</td>\n",
       "      <td>0.386087</td>\n",
       "      <td>1.279600</td>\n",
       "      <td>0.185456</td>\n",
       "      <td>0.148696</td>\n",
       "      <td>0.190532</td>\n",
       "      <td>0.108303</td>\n",
       "      <td>0.145330</td>\n",
       "      <td>0.176213</td>\n",
       "      <td>0.132560</td>\n",
       "      <td>0.117760</td>\n",
       "      <td>0.244817</td>\n",
       "      <td>0.142445</td>\n",
       "      <td>0.510472</td>\n",
       "      <td>0.255343</td>\n",
       "      <td>1.663550</td>\n",
       "      <td>2.016831</td>\n",
       "      <td>0.108196</td>\n",
       "      <td>0.996848</td>\n",
       "      <td>0.846709</td>\n",
       "      <td>0.193685</td>\n",
       "      <td>0.118508</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.108303</td>\n",
       "      <td>0.106219</td>\n",
       "      <td>0.435777</td>\n",
       "      <td>0.111883</td>\n",
       "      <td>0.133362</td>\n",
       "      <td>0.127431</td>\n",
       "      <td>1.926427</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>309_4</td>\n",
       "      <td>0.442107</td>\n",
       "      <td>0.617076</td>\n",
       "      <td>0.358626</td>\n",
       "      <td>2.466947</td>\n",
       "      <td>4.979503</td>\n",
       "      <td>0.222886</td>\n",
       "      <td>0.176463</td>\n",
       "      <td>2.152301</td>\n",
       "      <td>0.207004</td>\n",
       "      <td>1.595086</td>\n",
       "      <td>0.583277</td>\n",
       "      <td>0.296729</td>\n",
       "      <td>0.377087</td>\n",
       "      <td>0.313832</td>\n",
       "      <td>0.875390</td>\n",
       "      <td>0.520293</td>\n",
       "      <td>1.566852</td>\n",
       "      <td>2.132754</td>\n",
       "      <td>0.477671</td>\n",
       "      <td>0.727705</td>\n",
       "      <td>0.385639</td>\n",
       "      <td>0.362970</td>\n",
       "      <td>0.179449</td>\n",
       "      <td>1.286277</td>\n",
       "      <td>2.970137</td>\n",
       "      <td>1.419710</td>\n",
       "      <td>0.259536</td>\n",
       "      <td>0.279218</td>\n",
       "      <td>0.734492</td>\n",
       "      <td>0.162210</td>\n",
       "      <td>0.410615</td>\n",
       "      <td>2.500204</td>\n",
       "      <td>0.344509</td>\n",
       "      <td>0.429211</td>\n",
       "      <td>0.330121</td>\n",
       "      <td>0.746980</td>\n",
       "      <td>0.546763</td>\n",
       "      <td>0.386860</td>\n",
       "      <td>0.547849</td>\n",
       "      <td>0.366771</td>\n",
       "      <td>0.328492</td>\n",
       "      <td>0.442650</td>\n",
       "      <td>0.398534</td>\n",
       "      <td>0.161768</td>\n",
       "      <td>0.760234</td>\n",
       "      <td>0.184169</td>\n",
       "      <td>1.612382</td>\n",
       "      <td>0.296382</td>\n",
       "      <td>0.290680</td>\n",
       "      <td>1.198765</td>\n",
       "      <td>0.159799</td>\n",
       "      <td>0.166112</td>\n",
       "      <td>0.185323</td>\n",
       "      <td>0.103184</td>\n",
       "      <td>0.140656</td>\n",
       "      <td>0.163804</td>\n",
       "      <td>0.123210</td>\n",
       "      <td>0.117439</td>\n",
       "      <td>0.234947</td>\n",
       "      <td>0.145068</td>\n",
       "      <td>0.430996</td>\n",
       "      <td>0.251103</td>\n",
       "      <td>1.484624</td>\n",
       "      <td>1.957233</td>\n",
       "      <td>0.119883</td>\n",
       "      <td>0.990225</td>\n",
       "      <td>0.833277</td>\n",
       "      <td>0.192112</td>\n",
       "      <td>0.132781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.103184</td>\n",
       "      <td>0.111262</td>\n",
       "      <td>0.391691</td>\n",
       "      <td>0.130405</td>\n",
       "      <td>0.147444</td>\n",
       "      <td>0.146901</td>\n",
       "      <td>1.700563</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>309_5</td>\n",
       "      <td>0.434940</td>\n",
       "      <td>0.617430</td>\n",
       "      <td>0.358802</td>\n",
       "      <td>2.365785</td>\n",
       "      <td>4.718679</td>\n",
       "      <td>0.213106</td>\n",
       "      <td>0.173627</td>\n",
       "      <td>2.134014</td>\n",
       "      <td>0.192158</td>\n",
       "      <td>1.504230</td>\n",
       "      <td>0.550960</td>\n",
       "      <td>0.286961</td>\n",
       "      <td>0.363502</td>\n",
       "      <td>0.277964</td>\n",
       "      <td>0.864912</td>\n",
       "      <td>0.507990</td>\n",
       "      <td>1.480059</td>\n",
       "      <td>2.013697</td>\n",
       "      <td>0.483416</td>\n",
       "      <td>0.687794</td>\n",
       "      <td>0.367531</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>0.174836</td>\n",
       "      <td>1.324695</td>\n",
       "      <td>2.896334</td>\n",
       "      <td>1.359876</td>\n",
       "      <td>0.250705</td>\n",
       "      <td>0.273667</td>\n",
       "      <td>0.702699</td>\n",
       "      <td>0.154827</td>\n",
       "      <td>0.398550</td>\n",
       "      <td>2.456560</td>\n",
       "      <td>0.329126</td>\n",
       "      <td>0.408755</td>\n",
       "      <td>0.313415</td>\n",
       "      <td>0.691956</td>\n",
       "      <td>0.536860</td>\n",
       "      <td>0.360816</td>\n",
       "      <td>0.512824</td>\n",
       "      <td>0.351551</td>\n",
       "      <td>0.312206</td>\n",
       "      <td>0.419095</td>\n",
       "      <td>0.393447</td>\n",
       "      <td>0.160200</td>\n",
       "      <td>0.768113</td>\n",
       "      <td>0.185718</td>\n",
       "      <td>1.645807</td>\n",
       "      <td>0.296829</td>\n",
       "      <td>0.309345</td>\n",
       "      <td>1.206995</td>\n",
       "      <td>0.164650</td>\n",
       "      <td>0.160687</td>\n",
       "      <td>0.188221</td>\n",
       "      <td>0.104784</td>\n",
       "      <td>0.141983</td>\n",
       "      <td>0.167710</td>\n",
       "      <td>0.136838</td>\n",
       "      <td>0.116048</td>\n",
       "      <td>0.255528</td>\n",
       "      <td>0.140871</td>\n",
       "      <td>0.481227</td>\n",
       "      <td>0.251773</td>\n",
       "      <td>1.534835</td>\n",
       "      <td>2.009109</td>\n",
       "      <td>0.119524</td>\n",
       "      <td>0.997775</td>\n",
       "      <td>0.878668</td>\n",
       "      <td>0.205604</td>\n",
       "      <td>0.129954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.104784</td>\n",
       "      <td>0.110694</td>\n",
       "      <td>0.434154</td>\n",
       "      <td>0.118481</td>\n",
       "      <td>0.140314</td>\n",
       "      <td>0.148380</td>\n",
       "      <td>1.839730</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  MouseID  DYRK1A_N   ITSN1_N    BDNF_N     NR1_N    NR2A_N    pAKT_N  \\\n",
       "0   309_1  0.503644  0.747193  0.430175  2.816329  5.990152  0.218830   \n",
       "1   309_2  0.514617  0.689064  0.411770  2.789514  5.685038  0.211636   \n",
       "2   309_3  0.509183  0.730247  0.418309  2.687201  5.622059  0.209011   \n",
       "3   309_4  0.442107  0.617076  0.358626  2.466947  4.979503  0.222886   \n",
       "4   309_5  0.434940  0.617430  0.358802  2.365785  4.718679  0.213106   \n",
       "\n",
       "    pBRAF_N  pCAMKII_N   pCREB_N    pELK_N    pERK_N    pJNK_N    PKCA_N  \\\n",
       "0  0.177565   2.373744  0.232224  1.750936  0.687906  0.306382  0.402698   \n",
       "1  0.172817   2.292150  0.226972  1.596377  0.695006  0.299051  0.385987   \n",
       "2  0.175722   2.283337  0.230247  1.561316  0.677348  0.291276  0.381002   \n",
       "3  0.176463   2.152301  0.207004  1.595086  0.583277  0.296729  0.377087   \n",
       "4  0.173627   2.134014  0.192158  1.504230  0.550960  0.286961  0.363502   \n",
       "\n",
       "     pMEK_N    pNR1_N   pNR2A_N   pNR2B_N  pPKCAB_N    pRSK_N     AKT_N  \\\n",
       "0  0.296927  1.022060  0.605673  1.877684  2.308745  0.441599  0.859366   \n",
       "1  0.281319  0.956676  0.587559  1.725774  2.043037  0.445222  0.834659   \n",
       "2  0.281710  1.003635  0.602449  1.731873  2.017984  0.467668  0.814329   \n",
       "3  0.313832  0.875390  0.520293  1.566852  2.132754  0.477671  0.727705   \n",
       "4  0.277964  0.864912  0.507990  1.480059  2.013697  0.483416  0.687794   \n",
       "\n",
       "     BRAF_N  CAMKII_N    CREB_N     ELK_N     ERK_N   GSK3B_N     JNK_N  \\\n",
       "0  0.416289  0.369608  0.178944  1.866358  3.685247  1.537227  0.264526   \n",
       "1  0.400364  0.356178  0.173680  1.761047  3.485287  1.509249  0.255727   \n",
       "2  0.399847  0.368089  0.173905  1.765544  3.571456  1.501244  0.259614   \n",
       "3  0.385639  0.362970  0.179449  1.286277  2.970137  1.419710  0.259536   \n",
       "4  0.367531  0.355311  0.174836  1.324695  2.896334  1.359876  0.250705   \n",
       "\n",
       "      MEK_N    TRKA_N     RSK_N     APP_N  Bcatenin_N    SOD1_N    MTOR_N  \\\n",
       "0  0.319677  0.813866  0.165846  0.453910    3.037621  0.369510  0.458539   \n",
       "1  0.304419  0.780504  0.157194  0.430940    2.921882  0.342279  0.423560   \n",
       "2  0.311747  0.785154  0.160895  0.423187    2.944136  0.343696  0.425005   \n",
       "3  0.279218  0.734492  0.162210  0.410615    2.500204  0.344509  0.429211   \n",
       "4  0.273667  0.702699  0.154827  0.398550    2.456560  0.329126  0.408755   \n",
       "\n",
       "      P38_N   pMTOR_N   DSCR1_N   AMPKA_N    NR2B_N   pNUMB_N  RAPTOR_N  \\\n",
       "0  0.335336  0.825192  0.576916  0.448099  0.586271  0.394721  0.339571   \n",
       "1  0.324835  0.761718  0.545097  0.420876  0.545097  0.368255  0.321959   \n",
       "2  0.324852  0.757031  0.543620  0.404630  0.552994  0.363880  0.313086   \n",
       "3  0.330121  0.746980  0.546763  0.386860  0.547849  0.366771  0.328492   \n",
       "4  0.313415  0.691956  0.536860  0.360816  0.512824  0.351551  0.312206   \n",
       "\n",
       "    TIAM1_N  pP70S6_N    NUMB_N   P70S6_N  pGSK3B_N   pPKCG_N    CDK5_N  \\\n",
       "0  0.482864  0.294170  0.182150  0.842725  0.192608  1.443091  0.294700   \n",
       "1  0.454519  0.276431  0.182086  0.847615  0.194815  1.439460  0.294060   \n",
       "2  0.447197  0.256648  0.184388  0.856166  0.200737  1.524364  0.301881   \n",
       "3  0.442650  0.398534  0.161768  0.760234  0.184169  1.612382  0.296382   \n",
       "4  0.419095  0.393447  0.160200  0.768113  0.185718  1.645807  0.296829   \n",
       "\n",
       "       S6_N  ADARB1_N  AcetylH3K9_N    RRP1_N     BAX_N     ARC_N   ERBB4_N  \\\n",
       "0  0.354605  1.339070      0.170119  0.159102  0.188852  0.106305  0.144989   \n",
       "1  0.354548  1.306323      0.171427  0.158129  0.184570  0.106592  0.150471   \n",
       "2  0.386087  1.279600      0.185456  0.148696  0.190532  0.108303  0.145330   \n",
       "3  0.290680  1.198765      0.159799  0.166112  0.185323  0.103184  0.140656   \n",
       "4  0.309345  1.206995      0.164650  0.160687  0.188221  0.104784  0.141983   \n",
       "\n",
       "     nNOS_N     Tau_N    GFAP_N   GluR3_N   GluR4_N    IL1B_N   P3525_N  \\\n",
       "0  0.176668  0.125190  0.115291  0.228043  0.142756  0.430957  0.247538   \n",
       "1  0.178309  0.134275  0.118235  0.238073  0.142037  0.457156  0.257632   \n",
       "2  0.176213  0.132560  0.117760  0.244817  0.142445  0.510472  0.255343   \n",
       "3  0.163804  0.123210  0.117439  0.234947  0.145068  0.430996  0.251103   \n",
       "4  0.167710  0.136838  0.116048  0.255528  0.140871  0.481227  0.251773   \n",
       "\n",
       "   pCASP9_N   PSD95_N    SNCA_N  Ubiquitin_N  pGSK3B_Tyr216_N     SHH_N  \\\n",
       "0  1.603310  2.014875  0.108234     1.044979         0.831557  0.188852   \n",
       "1  1.671738  2.004605  0.109749     1.009883         0.849270  0.200404   \n",
       "2  1.663550  2.016831  0.108196     0.996848         0.846709  0.193685   \n",
       "3  1.484624  1.957233  0.119883     0.990225         0.833277  0.192112   \n",
       "4  1.534835  2.009109  0.119524     0.997775         0.878668  0.205604   \n",
       "\n",
       "      BAD_N  BCL2_N     pS6_N   pCFOS_N     SYP_N  H3AcK18_N    EGR1_N  \\\n",
       "0  0.122652     NaN  0.106305  0.108336  0.427099   0.114783  0.131790   \n",
       "1  0.116682     NaN  0.106592  0.104315  0.441581   0.111974  0.135103   \n",
       "2  0.118508     NaN  0.108303  0.106219  0.435777   0.111883  0.133362   \n",
       "3  0.132781     NaN  0.103184  0.111262  0.391691   0.130405  0.147444   \n",
       "4  0.129954     NaN  0.104784  0.110694  0.434154   0.118481  0.140314   \n",
       "\n",
       "   H3MeK4_N    CaNA_N Genotype  Treatment Behavior  class  \n",
       "0  0.128186  1.675652  Control  Memantine      C/S      0  \n",
       "1  0.131119  1.743610  Control  Memantine      C/S      0  \n",
       "2  0.127431  1.926427  Control  Memantine      C/S      0  \n",
       "3  0.146901  1.700563  Control  Memantine      C/S      0  \n",
       "4  0.148380  1.839730  Control  Memantine      C/S      0  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'] = df['class'].map(class_mapping)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:81]\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate data into training and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, \n",
    "                     test_size=0.3,\n",
    "                     stratify=y,\n",
    "                     random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "to_be_removed = ['MouseID','Genotype','Treatment','Behavior','BAD_N','BCL2_N','pCFOS_N','H3AcK18_N','EGR1_N','H3MeK4_N','DYRK1A_N','ITSN1_N','BDNF_N','NR1_N','NR2A_N','pAKT_N','pBRAF_N','pCAMKII_N','pCREB_N','pELK_N','pJNK_N','PKCA_N','pMEK_N','pNR1_N','pNR2A_N','pNR2B_N','pPKCAB_N','pRSK_N','AKT_N','BRAF_N','CAMKII_N','CREB_N','ELK_N','ERK_N','GSK3B_N','JNK_N','MEK_N','TRKA_N','RSK_N','Bcatenin_N','MTOR_N','P38_N','pMTOR_N','DSCR1_N','AMPKA_N','NR2B_N','pNUMB_N','RAPTOR_N','TIAM1_N','pP70S6_N','NUMB_N','P70S6_N','pGSK3B_N','CDK5_N','S6_N','ADARB1_N','AcetylH3K9_N','RRP1_N','BAX_N','ARC_N','ERBB4_N','nNOS_N','Tau_N','GFAP_N','GluR3_N','GluR4_N','IL1B_N','P3525_N','pCASP9_N','PSD95_N','SNCA_N','pGSK3B_Tyr216_N','SHH_N','pS6_N','SYP_N']\n",
    "\n",
    "##Feature selection performed from project 01\n",
    "numeric_features = ['SOD1_N','pPKCG_N','pERK_N','APP_N','CaNA_N','Ubiquitin_N']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reference:\n",
    "##https://www.mikulskibartosz.name/preprocessing-the-input-pandas-dataframe-using-columntransformer-in-scikit-learn/\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    remainder = 'passthrough',\n",
    "    transformers=[\n",
    "        ('numeric', numeric_transformer, numeric_features),\n",
    "        ('remove', 'drop', to_be_removed)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.61711514, -0.45575874,  0.41276943, ...,  1.06590091,\n",
       "        -1.11983482,  0.        ],\n",
       "       [-0.71433269, -0.46203864,  0.43336635, ...,  1.28029118,\n",
       "        -1.32211812,  0.        ],\n",
       "       [-0.70927411, -0.31518956,  0.38214129, ...,  1.85703831,\n",
       "        -1.39725041,  0.        ],\n",
       "       ...,\n",
       "       [ 0.4264525 ,  1.59413441, -0.84295284, ...,  0.29352469,\n",
       "         0.01716445,  7.        ],\n",
       "       [ 0.55713637,  1.64847672, -0.91372574, ...,  0.2089962 ,\n",
       "         0.35620188,  7.        ],\n",
       "       [ 0.87279308,  1.64020927, -0.78173889, ...,  0.10478825,\n",
       "         0.16051442,  7.        ]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7103174603174603\n",
      "Test Accuracy: 0.691\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    LogisticRegression(random_state=1, solver='newton-cg', multi_class='multinomial'))\n",
    "    \n",
    "a = model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = a.predict(X_test)\n",
    "\n",
    "print('Training accuracy:', a.score(X_train, y_train))\n",
    "print('Test Accuracy: %.3f' % a.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Test Accuracy: 0.895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model1 = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestClassifier(criterion='gini',\n",
    "                           n_estimators=50, \n",
    "                           random_state=1,\n",
    "                           n_jobs=2))\n",
    "b = model1.fit(X_train,y_train)\n",
    "y_pred1 = b.predict(X_test)\n",
    "\n",
    "print('Training accuracy:', b.score(X_train, y_train))\n",
    "print('Test Accuracy: %.3f' % b.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.843915343915344\n",
      "Test accuracy: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model2 = make_pipeline(\n",
    "    preprocessor,\n",
    "    KNeighborsClassifier(n_neighbors=15))\n",
    "\n",
    "c = model2.fit(X_train,y_train)\n",
    "y_pred2 = c.predict(X_test)\n",
    "\n",
    "print('Training accuracy:', c.score(X_train, y_train))\n",
    "print('Test accuracy:', c.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "Based on all the metrics from the classification algorithms, the accuracies are same as those compared to Project 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part2:\n",
    "\n",
    "In order to perform a more equal comparison of your models' performance, and to ensure that we have the best set of hyperparameters in use, implement nested cross-validation for each algorithm using the GridSearchCV class and cross_val_score function. For each of your algorithms that you are comparing, create a list of values for each applicable hyperparameter and provide it as the param_grid for the GridSearchCV class. Compare this nested cross-validation technique to the model comparison you performed in project 01. Are your findings different now that the models are tuned and compared in an unbiased manner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.736701754385965\n",
      "{'logisticregression__C': 100.0}\n",
      "CV accuracy: 0.730 +/- 0.044\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "param_grid={'logisticregression__C':[0.001,0.01,0.1,1.0,10.0,100.0]}\n",
    "\n",
    "gs = GridSearchCV(estimator=model, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='accuracy', \n",
    "                  refit=True,\n",
    "                  cv=10,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "\n",
    "#model.get_params().keys()\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "\n",
    "scores = cross_val_score(clf, X_train, y_train, \n",
    "                         scoring='accuracy', cv=5)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores),\n",
    "                                      np.std(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8678421052631577\n",
      "{'randomforestclassifier__criterion': 'entropy', 'randomforestclassifier__max_depth': 8, 'randomforestclassifier__max_features': 'auto', 'randomforestclassifier__n_estimators': 70}\n",
      "CV accuracy: 0.866 +/- 0.023\n"
     ]
    }
   ],
   "source": [
    "##Reference: The parameters were refered from the following link\n",
    "##https://www.kaggle.com/sociopath00/random-forest-using-gridsearchcv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid1 = { \n",
    "    'randomforestclassifier__n_estimators': list(range(10,101,10)),\n",
    "    'randomforestclassifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'randomforestclassifier__max_depth' : [4,5,6,7,8],\n",
    "    'randomforestclassifier__criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "gs1 = GridSearchCV(estimator=model1, \n",
    "                  param_grid=param_grid1, \n",
    "                  scoring='accuracy', \n",
    "                  refit=True,\n",
    "                  cv=10,\n",
    "                  n_jobs=4)\n",
    "\n",
    "\n",
    "#model1.get_params().keys()\n",
    "\n",
    "gs1 = gs1.fit(X_train, y_train)\n",
    "print(gs1.best_score_)\n",
    "print(gs1.best_params_)\n",
    "\n",
    "scores = cross_val_score(gs1, X_train, y_train, \n",
    "                         scoring='accuracy', cv=5)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores),\n",
    "                                      np.std(scores)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.894157894736842\n",
      "{'kneighborsclassifier__metric': 'manhattan', 'kneighborsclassifier__n_neighbors': 3, 'kneighborsclassifier__weights': 'uniform'}\n",
      "CV accuracy: 0.890 +/- 0.007\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid2 = {\n",
    "    'kneighborsclassifier__n_neighbors':[3, 5, 7, 9, 11,15],\n",
    "    'kneighborsclassifier__weights':['uniform'],\n",
    "    'kneighborsclassifier__metric':['euclidean','manhattan']}\n",
    "\n",
    "gs2 = GridSearchCV(estimator=model2, \n",
    "                  param_grid=param_grid2, \n",
    "                  scoring='accuracy', \n",
    "                  refit=True,\n",
    "                  cv=10,\n",
    "                  n_jobs=4)\n",
    "\n",
    "\n",
    "#model2.get_params().keys()\n",
    "\n",
    "gs2 = gs2.fit(X_train, y_train)\n",
    "print(gs2.best_score_)\n",
    "print(gs2.best_params_)\n",
    "\n",
    "scores = cross_val_score(gs2, X_train, y_train, \n",
    "                         scoring='accuracy', cv=5)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores),\n",
    "                                      np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "By optimizing the hyperparameters, it was observed that there was a slight improvement in accuracy with regards to Random Forest and KNN neighbours. There was a 100% accuracy in training datset when Random Forest model was used. This seemed to have occured by overfitting. However it is seen that there is a decrease in accyracy by using the appropriate hyperparameter. There wasn't any improvement in logistic regression because the same optimized parameter was used from project01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part3 \n",
    "\n",
    "Create a confusion matrix showing the TP, FP, TN, and FN values for each of the three optimized models that were created using nested cross validation in step 2. Visualize the confusion matrix using a similar technique to the one that was used in the ch06 example Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31  9  0  0  1  4  0  0]\n",
      " [10 27  0  0  1  3  0  0]\n",
      " [ 0  0 38  4  0  0  2  1]\n",
      " [ 0  0  3 30  0  0  5  2]\n",
      " [ 8  2  0  0 24  6  0  0]\n",
      " [ 3  0  0  0 12 17  0  0]\n",
      " [ 0  0  7  2  0  0 31  0]\n",
      " [ 0  0  1  2  0  0  2 36]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "##logistic regression\n",
    "gs.fit(X_train, y_train)\n",
    "y_pred = gs.predict(X_test)\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.72\n",
      "\n",
      "Micro Precision: 0.72\n",
      "Micro Recall: 0.72\n",
      "Micro F1-score: 0.72\n",
      "\n",
      "Macro Precision: 0.72\n",
      "Macro Recall: 0.72\n",
      "Macro F1-score: 0.72\n",
      "\n",
      "Weighted Precision: 0.72\n",
      "Weighted Recall: 0.72\n",
      "Weighted F1-score: 0.72\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 1       0.60      0.69      0.64        45\n",
      "     Class 2       0.71      0.66      0.68        41\n",
      "     Class 3       0.78      0.84      0.81        45\n",
      "     Class 4       0.79      0.75      0.77        40\n",
      "     Class 5       0.63      0.60      0.62        40\n",
      "     Class 6       0.57      0.53      0.55        32\n",
      "     Class 7       0.78      0.78      0.78        40\n",
      "     Class 8       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.72       324\n",
      "   macro avg       0.72      0.72      0.72       324\n",
      "weighted avg       0.72      0.72      0.72       324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_test, y_pred, target_names=['Class 1', 'Class 2', 'Class 3','Class 4','Class 5', 'Class 6', 'Class 7', 'Class 8']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39  5  0  0  0  1  0  0]\n",
      " [ 3 37  0  0  1  0  0  0]\n",
      " [ 0  0 43  0  0  0  2  0]\n",
      " [ 0  0  0 38  0  0  1  1]\n",
      " [ 4  3  0  0 32  1  0  0]\n",
      " [ 6  1  0  0  3 22  0  0]\n",
      " [ 0  0  1  3  0  0 36  0]\n",
      " [ 0  0  1  0  0  0  2 38]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "##Random forest\n",
    "gs1.fit(X_train, y_train)\n",
    "y_pred1 = gs1.predict(X_test)\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred1)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.88\n",
      "\n",
      "Micro Precision: 0.88\n",
      "Micro Recall: 0.88\n",
      "Micro F1-score: 0.72\n",
      "\n",
      "Macro Precision: 0.89\n",
      "Macro Recall: 0.87\n",
      "Macro F1-score: 0.88\n",
      "\n",
      "Weighted Precision: 0.89\n",
      "Weighted Recall: 0.88\n",
      "Weighted F1-score: 0.88\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 1       0.75      0.87      0.80        45\n",
      "     Class 2       0.80      0.90      0.85        41\n",
      "     Class 3       0.96      0.96      0.96        45\n",
      "     Class 4       0.93      0.95      0.94        40\n",
      "     Class 5       0.89      0.80      0.84        40\n",
      "     Class 6       0.92      0.69      0.79        32\n",
      "     Class 7       0.88      0.90      0.89        40\n",
      "     Class 8       0.97      0.93      0.95        41\n",
      "\n",
      "    accuracy                           0.88       324\n",
      "   macro avg       0.89      0.87      0.88       324\n",
      "weighted avg       0.89      0.88      0.88       324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, y_pred1)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred1, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred1, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred1, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred1, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred1, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred1, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred1, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred1, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_test, y_pred1, target_names=['Class 1', 'Class 2', 'Class 3','Class 4','Class 5', 'Class 6', 'Class 7', 'Class 8']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43  1  0  0  0  1  0  0]\n",
      " [ 4 35  0  0  1  1  0  0]\n",
      " [ 0  0 45  0  0  0  0  0]\n",
      " [ 0  0  1 37  0  0  1  1]\n",
      " [ 4  1  0  0 34  1  0  0]\n",
      " [ 1  2  0  0  0 29  0  0]\n",
      " [ 0  0  1  2  0  0 37  0]\n",
      " [ 0  0  1  0  0  0  2 38]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "##KNN\n",
    "gs2.fit(X_train, y_train)\n",
    "y_pred2 = gs2.predict(X_test)\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred2)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.92\n",
      "\n",
      "Micro Precision: 0.92\n",
      "Micro Recall: 0.92\n",
      "Micro F1-score: 0.92\n",
      "\n",
      "Macro Precision: 0.92\n",
      "Macro Recall: 0.92\n",
      "Macro F1-score: 0.92\n",
      "\n",
      "Weighted Precision: 0.92\n",
      "Weighted Recall: 0.92\n",
      "Weighted F1-score: 0.92\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 1       0.83      0.96      0.89        45\n",
      "     Class 2       0.90      0.85      0.88        41\n",
      "     Class 3       0.94      1.00      0.97        45\n",
      "     Class 4       0.95      0.93      0.94        40\n",
      "     Class 5       0.97      0.85      0.91        40\n",
      "     Class 6       0.91      0.91      0.91        32\n",
      "     Class 7       0.93      0.93      0.93        40\n",
      "     Class 8       0.97      0.93      0.95        41\n",
      "\n",
      "    accuracy                           0.92       324\n",
      "   macro avg       0.92      0.92      0.92       324\n",
      "weighted avg       0.92      0.92      0.92       324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, y_pred2)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred2, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred2, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred2, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred2, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred2, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred2, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred2, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred2, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred2, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_test, y_pred2, target_names=['Class 1', 'Class 2', 'Class 3','Class 4','Class 5', 'Class 6', 'Class 7', 'Class 8']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation:\n",
      "\n",
      "ROC AUC: 0.96 (+/- 0.01) [Logistic regression]\n",
      "ROC AUC: 0.99 (+/- 0.00) [Random Forest]\n",
      "ROC AUC: 0.99 (+/- 0.00) [KNN]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf1 = LogisticRegression(penalty='l2', \n",
    "                          C=100.0,\n",
    "                          solver='newton-cg',\n",
    "                          multi_class='multinomial',\n",
    "                          random_state=1)\n",
    "\n",
    "clf2 = RandomForestClassifier(criterion= 'entropy',\n",
    "                               max_depth= 8, \n",
    "                               max_features= 'auto', \n",
    "                               n_estimators= 70)\n",
    "\n",
    "clf3 = KNeighborsClassifier(metric= 'manhattan',\n",
    "                            n_neighbors= 3, \n",
    "                            weights = 'uniform')\n",
    "\n",
    "pipe1 = Pipeline([['preprocess', preprocessor],\n",
    "                  ['clf1', clf1]])\n",
    "pipe2 = Pipeline([['preprocess', preprocessor],\n",
    "                  ['clf2', clf2]])\n",
    "pipe3 = Pipeline([['preprocess', preprocessor],\n",
    "                  ['clf3', clf3]])\n",
    "\n",
    "clf_labels = ['Logistic regression', 'Random Forest', 'KNN']\n",
    "\n",
    "print('10-fold cross validation:\\n')\n",
    "for clf, label in zip([pipe1, pipe2, pipe3], clf_labels):\n",
    "    scores = cross_val_score(estimator=clf,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=10,\n",
    "                             scoring='roc_auc_ovr')\n",
    "    print(\"ROC AUC: %0.2f (+/- %0.2f) [%s]\"\n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of ensemble models have outperformed significantly when compared to individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higuera, Clara; Gardiner, Katheleen J.; J. Cios, Krzysztof (2015): Protein expression levels of 77 proteins from Control and Down Syndrome mice. figshare. Dataset. https://doi.org/10.6084/m9.figshare.1421985.v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Data Collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MouseID</th>\n",
       "      <th>DYRK1A_N</th>\n",
       "      <th>ITSN1_N</th>\n",
       "      <th>BDNF_N</th>\n",
       "      <th>NR1_N</th>\n",
       "      <th>NR2A_N</th>\n",
       "      <th>pAKT_N</th>\n",
       "      <th>pBRAF_N</th>\n",
       "      <th>pCAMKII_N</th>\n",
       "      <th>pCREB_N</th>\n",
       "      <th>pELK_N</th>\n",
       "      <th>pERK_N</th>\n",
       "      <th>pJNK_N</th>\n",
       "      <th>PKCA_N</th>\n",
       "      <th>pMEK_N</th>\n",
       "      <th>pNR1_N</th>\n",
       "      <th>pNR2A_N</th>\n",
       "      <th>pNR2B_N</th>\n",
       "      <th>pPKCAB_N</th>\n",
       "      <th>pRSK_N</th>\n",
       "      <th>AKT_N</th>\n",
       "      <th>BRAF_N</th>\n",
       "      <th>CAMKII_N</th>\n",
       "      <th>CREB_N</th>\n",
       "      <th>ELK_N</th>\n",
       "      <th>ERK_N</th>\n",
       "      <th>GSK3B_N</th>\n",
       "      <th>JNK_N</th>\n",
       "      <th>MEK_N</th>\n",
       "      <th>TRKA_N</th>\n",
       "      <th>RSK_N</th>\n",
       "      <th>APP_N</th>\n",
       "      <th>Bcatenin_N</th>\n",
       "      <th>SOD1_N</th>\n",
       "      <th>MTOR_N</th>\n",
       "      <th>P38_N</th>\n",
       "      <th>pMTOR_N</th>\n",
       "      <th>DSCR1_N</th>\n",
       "      <th>AMPKA_N</th>\n",
       "      <th>NR2B_N</th>\n",
       "      <th>pNUMB_N</th>\n",
       "      <th>RAPTOR_N</th>\n",
       "      <th>TIAM1_N</th>\n",
       "      <th>pP70S6_N</th>\n",
       "      <th>NUMB_N</th>\n",
       "      <th>P70S6_N</th>\n",
       "      <th>pGSK3B_N</th>\n",
       "      <th>pPKCG_N</th>\n",
       "      <th>CDK5_N</th>\n",
       "      <th>S6_N</th>\n",
       "      <th>ADARB1_N</th>\n",
       "      <th>AcetylH3K9_N</th>\n",
       "      <th>RRP1_N</th>\n",
       "      <th>BAX_N</th>\n",
       "      <th>ARC_N</th>\n",
       "      <th>ERBB4_N</th>\n",
       "      <th>nNOS_N</th>\n",
       "      <th>Tau_N</th>\n",
       "      <th>GFAP_N</th>\n",
       "      <th>GluR3_N</th>\n",
       "      <th>GluR4_N</th>\n",
       "      <th>IL1B_N</th>\n",
       "      <th>P3525_N</th>\n",
       "      <th>pCASP9_N</th>\n",
       "      <th>PSD95_N</th>\n",
       "      <th>SNCA_N</th>\n",
       "      <th>Ubiquitin_N</th>\n",
       "      <th>pGSK3B_Tyr216_N</th>\n",
       "      <th>SHH_N</th>\n",
       "      <th>BAD_N</th>\n",
       "      <th>BCL2_N</th>\n",
       "      <th>pS6_N</th>\n",
       "      <th>pCFOS_N</th>\n",
       "      <th>SYP_N</th>\n",
       "      <th>H3AcK18_N</th>\n",
       "      <th>EGR1_N</th>\n",
       "      <th>H3MeK4_N</th>\n",
       "      <th>CaNA_N</th>\n",
       "      <th>Genotype</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>Behavior</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>309_1</td>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.747193</td>\n",
       "      <td>0.430175</td>\n",
       "      <td>2.816329</td>\n",
       "      <td>5.990152</td>\n",
       "      <td>0.218830</td>\n",
       "      <td>0.177565</td>\n",
       "      <td>2.373744</td>\n",
       "      <td>0.232224</td>\n",
       "      <td>1.750936</td>\n",
       "      <td>0.687906</td>\n",
       "      <td>0.306382</td>\n",
       "      <td>0.402698</td>\n",
       "      <td>0.296927</td>\n",
       "      <td>1.022060</td>\n",
       "      <td>0.605673</td>\n",
       "      <td>1.877684</td>\n",
       "      <td>2.308745</td>\n",
       "      <td>0.441599</td>\n",
       "      <td>0.859366</td>\n",
       "      <td>0.416289</td>\n",
       "      <td>0.369608</td>\n",
       "      <td>0.178944</td>\n",
       "      <td>1.866358</td>\n",
       "      <td>3.685247</td>\n",
       "      <td>1.537227</td>\n",
       "      <td>0.264526</td>\n",
       "      <td>0.319677</td>\n",
       "      <td>0.813866</td>\n",
       "      <td>0.165846</td>\n",
       "      <td>0.453910</td>\n",
       "      <td>3.037621</td>\n",
       "      <td>0.369510</td>\n",
       "      <td>0.458539</td>\n",
       "      <td>0.335336</td>\n",
       "      <td>0.825192</td>\n",
       "      <td>0.576916</td>\n",
       "      <td>0.448099</td>\n",
       "      <td>0.586271</td>\n",
       "      <td>0.394721</td>\n",
       "      <td>0.339571</td>\n",
       "      <td>0.482864</td>\n",
       "      <td>0.294170</td>\n",
       "      <td>0.182150</td>\n",
       "      <td>0.842725</td>\n",
       "      <td>0.192608</td>\n",
       "      <td>1.443091</td>\n",
       "      <td>0.294700</td>\n",
       "      <td>0.354605</td>\n",
       "      <td>1.339070</td>\n",
       "      <td>0.170119</td>\n",
       "      <td>0.159102</td>\n",
       "      <td>0.188852</td>\n",
       "      <td>0.106305</td>\n",
       "      <td>0.144989</td>\n",
       "      <td>0.176668</td>\n",
       "      <td>0.125190</td>\n",
       "      <td>0.115291</td>\n",
       "      <td>0.228043</td>\n",
       "      <td>0.142756</td>\n",
       "      <td>0.430957</td>\n",
       "      <td>0.247538</td>\n",
       "      <td>1.603310</td>\n",
       "      <td>2.014875</td>\n",
       "      <td>0.108234</td>\n",
       "      <td>1.044979</td>\n",
       "      <td>0.831557</td>\n",
       "      <td>0.188852</td>\n",
       "      <td>0.122652</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.106305</td>\n",
       "      <td>0.108336</td>\n",
       "      <td>0.427099</td>\n",
       "      <td>0.114783</td>\n",
       "      <td>0.131790</td>\n",
       "      <td>0.128186</td>\n",
       "      <td>1.675652</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>309_2</td>\n",
       "      <td>0.514617</td>\n",
       "      <td>0.689064</td>\n",
       "      <td>0.411770</td>\n",
       "      <td>2.789514</td>\n",
       "      <td>5.685038</td>\n",
       "      <td>0.211636</td>\n",
       "      <td>0.172817</td>\n",
       "      <td>2.292150</td>\n",
       "      <td>0.226972</td>\n",
       "      <td>1.596377</td>\n",
       "      <td>0.695006</td>\n",
       "      <td>0.299051</td>\n",
       "      <td>0.385987</td>\n",
       "      <td>0.281319</td>\n",
       "      <td>0.956676</td>\n",
       "      <td>0.587559</td>\n",
       "      <td>1.725774</td>\n",
       "      <td>2.043037</td>\n",
       "      <td>0.445222</td>\n",
       "      <td>0.834659</td>\n",
       "      <td>0.400364</td>\n",
       "      <td>0.356178</td>\n",
       "      <td>0.173680</td>\n",
       "      <td>1.761047</td>\n",
       "      <td>3.485287</td>\n",
       "      <td>1.509249</td>\n",
       "      <td>0.255727</td>\n",
       "      <td>0.304419</td>\n",
       "      <td>0.780504</td>\n",
       "      <td>0.157194</td>\n",
       "      <td>0.430940</td>\n",
       "      <td>2.921882</td>\n",
       "      <td>0.342279</td>\n",
       "      <td>0.423560</td>\n",
       "      <td>0.324835</td>\n",
       "      <td>0.761718</td>\n",
       "      <td>0.545097</td>\n",
       "      <td>0.420876</td>\n",
       "      <td>0.545097</td>\n",
       "      <td>0.368255</td>\n",
       "      <td>0.321959</td>\n",
       "      <td>0.454519</td>\n",
       "      <td>0.276431</td>\n",
       "      <td>0.182086</td>\n",
       "      <td>0.847615</td>\n",
       "      <td>0.194815</td>\n",
       "      <td>1.439460</td>\n",
       "      <td>0.294060</td>\n",
       "      <td>0.354548</td>\n",
       "      <td>1.306323</td>\n",
       "      <td>0.171427</td>\n",
       "      <td>0.158129</td>\n",
       "      <td>0.184570</td>\n",
       "      <td>0.106592</td>\n",
       "      <td>0.150471</td>\n",
       "      <td>0.178309</td>\n",
       "      <td>0.134275</td>\n",
       "      <td>0.118235</td>\n",
       "      <td>0.238073</td>\n",
       "      <td>0.142037</td>\n",
       "      <td>0.457156</td>\n",
       "      <td>0.257632</td>\n",
       "      <td>1.671738</td>\n",
       "      <td>2.004605</td>\n",
       "      <td>0.109749</td>\n",
       "      <td>1.009883</td>\n",
       "      <td>0.849270</td>\n",
       "      <td>0.200404</td>\n",
       "      <td>0.116682</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.106592</td>\n",
       "      <td>0.104315</td>\n",
       "      <td>0.441581</td>\n",
       "      <td>0.111974</td>\n",
       "      <td>0.135103</td>\n",
       "      <td>0.131119</td>\n",
       "      <td>1.743610</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>309_3</td>\n",
       "      <td>0.509183</td>\n",
       "      <td>0.730247</td>\n",
       "      <td>0.418309</td>\n",
       "      <td>2.687201</td>\n",
       "      <td>5.622059</td>\n",
       "      <td>0.209011</td>\n",
       "      <td>0.175722</td>\n",
       "      <td>2.283337</td>\n",
       "      <td>0.230247</td>\n",
       "      <td>1.561316</td>\n",
       "      <td>0.677348</td>\n",
       "      <td>0.291276</td>\n",
       "      <td>0.381002</td>\n",
       "      <td>0.281710</td>\n",
       "      <td>1.003635</td>\n",
       "      <td>0.602449</td>\n",
       "      <td>1.731873</td>\n",
       "      <td>2.017984</td>\n",
       "      <td>0.467668</td>\n",
       "      <td>0.814329</td>\n",
       "      <td>0.399847</td>\n",
       "      <td>0.368089</td>\n",
       "      <td>0.173905</td>\n",
       "      <td>1.765544</td>\n",
       "      <td>3.571456</td>\n",
       "      <td>1.501244</td>\n",
       "      <td>0.259614</td>\n",
       "      <td>0.311747</td>\n",
       "      <td>0.785154</td>\n",
       "      <td>0.160895</td>\n",
       "      <td>0.423187</td>\n",
       "      <td>2.944136</td>\n",
       "      <td>0.343696</td>\n",
       "      <td>0.425005</td>\n",
       "      <td>0.324852</td>\n",
       "      <td>0.757031</td>\n",
       "      <td>0.543620</td>\n",
       "      <td>0.404630</td>\n",
       "      <td>0.552994</td>\n",
       "      <td>0.363880</td>\n",
       "      <td>0.313086</td>\n",
       "      <td>0.447197</td>\n",
       "      <td>0.256648</td>\n",
       "      <td>0.184388</td>\n",
       "      <td>0.856166</td>\n",
       "      <td>0.200737</td>\n",
       "      <td>1.524364</td>\n",
       "      <td>0.301881</td>\n",
       "      <td>0.386087</td>\n",
       "      <td>1.279600</td>\n",
       "      <td>0.185456</td>\n",
       "      <td>0.148696</td>\n",
       "      <td>0.190532</td>\n",
       "      <td>0.108303</td>\n",
       "      <td>0.145330</td>\n",
       "      <td>0.176213</td>\n",
       "      <td>0.132560</td>\n",
       "      <td>0.117760</td>\n",
       "      <td>0.244817</td>\n",
       "      <td>0.142445</td>\n",
       "      <td>0.510472</td>\n",
       "      <td>0.255343</td>\n",
       "      <td>1.663550</td>\n",
       "      <td>2.016831</td>\n",
       "      <td>0.108196</td>\n",
       "      <td>0.996848</td>\n",
       "      <td>0.846709</td>\n",
       "      <td>0.193685</td>\n",
       "      <td>0.118508</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.108303</td>\n",
       "      <td>0.106219</td>\n",
       "      <td>0.435777</td>\n",
       "      <td>0.111883</td>\n",
       "      <td>0.133362</td>\n",
       "      <td>0.127431</td>\n",
       "      <td>1.926427</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>309_4</td>\n",
       "      <td>0.442107</td>\n",
       "      <td>0.617076</td>\n",
       "      <td>0.358626</td>\n",
       "      <td>2.466947</td>\n",
       "      <td>4.979503</td>\n",
       "      <td>0.222886</td>\n",
       "      <td>0.176463</td>\n",
       "      <td>2.152301</td>\n",
       "      <td>0.207004</td>\n",
       "      <td>1.595086</td>\n",
       "      <td>0.583277</td>\n",
       "      <td>0.296729</td>\n",
       "      <td>0.377087</td>\n",
       "      <td>0.313832</td>\n",
       "      <td>0.875390</td>\n",
       "      <td>0.520293</td>\n",
       "      <td>1.566852</td>\n",
       "      <td>2.132754</td>\n",
       "      <td>0.477671</td>\n",
       "      <td>0.727705</td>\n",
       "      <td>0.385639</td>\n",
       "      <td>0.362970</td>\n",
       "      <td>0.179449</td>\n",
       "      <td>1.286277</td>\n",
       "      <td>2.970137</td>\n",
       "      <td>1.419710</td>\n",
       "      <td>0.259536</td>\n",
       "      <td>0.279218</td>\n",
       "      <td>0.734492</td>\n",
       "      <td>0.162210</td>\n",
       "      <td>0.410615</td>\n",
       "      <td>2.500204</td>\n",
       "      <td>0.344509</td>\n",
       "      <td>0.429211</td>\n",
       "      <td>0.330121</td>\n",
       "      <td>0.746980</td>\n",
       "      <td>0.546763</td>\n",
       "      <td>0.386860</td>\n",
       "      <td>0.547849</td>\n",
       "      <td>0.366771</td>\n",
       "      <td>0.328492</td>\n",
       "      <td>0.442650</td>\n",
       "      <td>0.398534</td>\n",
       "      <td>0.161768</td>\n",
       "      <td>0.760234</td>\n",
       "      <td>0.184169</td>\n",
       "      <td>1.612382</td>\n",
       "      <td>0.296382</td>\n",
       "      <td>0.290680</td>\n",
       "      <td>1.198765</td>\n",
       "      <td>0.159799</td>\n",
       "      <td>0.166112</td>\n",
       "      <td>0.185323</td>\n",
       "      <td>0.103184</td>\n",
       "      <td>0.140656</td>\n",
       "      <td>0.163804</td>\n",
       "      <td>0.123210</td>\n",
       "      <td>0.117439</td>\n",
       "      <td>0.234947</td>\n",
       "      <td>0.145068</td>\n",
       "      <td>0.430996</td>\n",
       "      <td>0.251103</td>\n",
       "      <td>1.484624</td>\n",
       "      <td>1.957233</td>\n",
       "      <td>0.119883</td>\n",
       "      <td>0.990225</td>\n",
       "      <td>0.833277</td>\n",
       "      <td>0.192112</td>\n",
       "      <td>0.132781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.103184</td>\n",
       "      <td>0.111262</td>\n",
       "      <td>0.391691</td>\n",
       "      <td>0.130405</td>\n",
       "      <td>0.147444</td>\n",
       "      <td>0.146901</td>\n",
       "      <td>1.700563</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>309_5</td>\n",
       "      <td>0.434940</td>\n",
       "      <td>0.617430</td>\n",
       "      <td>0.358802</td>\n",
       "      <td>2.365785</td>\n",
       "      <td>4.718679</td>\n",
       "      <td>0.213106</td>\n",
       "      <td>0.173627</td>\n",
       "      <td>2.134014</td>\n",
       "      <td>0.192158</td>\n",
       "      <td>1.504230</td>\n",
       "      <td>0.550960</td>\n",
       "      <td>0.286961</td>\n",
       "      <td>0.363502</td>\n",
       "      <td>0.277964</td>\n",
       "      <td>0.864912</td>\n",
       "      <td>0.507990</td>\n",
       "      <td>1.480059</td>\n",
       "      <td>2.013697</td>\n",
       "      <td>0.483416</td>\n",
       "      <td>0.687794</td>\n",
       "      <td>0.367531</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>0.174836</td>\n",
       "      <td>1.324695</td>\n",
       "      <td>2.896334</td>\n",
       "      <td>1.359876</td>\n",
       "      <td>0.250705</td>\n",
       "      <td>0.273667</td>\n",
       "      <td>0.702699</td>\n",
       "      <td>0.154827</td>\n",
       "      <td>0.398550</td>\n",
       "      <td>2.456560</td>\n",
       "      <td>0.329126</td>\n",
       "      <td>0.408755</td>\n",
       "      <td>0.313415</td>\n",
       "      <td>0.691956</td>\n",
       "      <td>0.536860</td>\n",
       "      <td>0.360816</td>\n",
       "      <td>0.512824</td>\n",
       "      <td>0.351551</td>\n",
       "      <td>0.312206</td>\n",
       "      <td>0.419095</td>\n",
       "      <td>0.393447</td>\n",
       "      <td>0.160200</td>\n",
       "      <td>0.768113</td>\n",
       "      <td>0.185718</td>\n",
       "      <td>1.645807</td>\n",
       "      <td>0.296829</td>\n",
       "      <td>0.309345</td>\n",
       "      <td>1.206995</td>\n",
       "      <td>0.164650</td>\n",
       "      <td>0.160687</td>\n",
       "      <td>0.188221</td>\n",
       "      <td>0.104784</td>\n",
       "      <td>0.141983</td>\n",
       "      <td>0.167710</td>\n",
       "      <td>0.136838</td>\n",
       "      <td>0.116048</td>\n",
       "      <td>0.255528</td>\n",
       "      <td>0.140871</td>\n",
       "      <td>0.481227</td>\n",
       "      <td>0.251773</td>\n",
       "      <td>1.534835</td>\n",
       "      <td>2.009109</td>\n",
       "      <td>0.119524</td>\n",
       "      <td>0.997775</td>\n",
       "      <td>0.878668</td>\n",
       "      <td>0.205604</td>\n",
       "      <td>0.129954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.104784</td>\n",
       "      <td>0.110694</td>\n",
       "      <td>0.434154</td>\n",
       "      <td>0.118481</td>\n",
       "      <td>0.140314</td>\n",
       "      <td>0.148380</td>\n",
       "      <td>1.839730</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  MouseID  DYRK1A_N   ITSN1_N    BDNF_N     NR1_N    NR2A_N    pAKT_N  \\\n",
       "0   309_1  0.503644  0.747193  0.430175  2.816329  5.990152  0.218830   \n",
       "1   309_2  0.514617  0.689064  0.411770  2.789514  5.685038  0.211636   \n",
       "2   309_3  0.509183  0.730247  0.418309  2.687201  5.622059  0.209011   \n",
       "3   309_4  0.442107  0.617076  0.358626  2.466947  4.979503  0.222886   \n",
       "4   309_5  0.434940  0.617430  0.358802  2.365785  4.718679  0.213106   \n",
       "\n",
       "    pBRAF_N  pCAMKII_N   pCREB_N    pELK_N    pERK_N    pJNK_N    PKCA_N  \\\n",
       "0  0.177565   2.373744  0.232224  1.750936  0.687906  0.306382  0.402698   \n",
       "1  0.172817   2.292150  0.226972  1.596377  0.695006  0.299051  0.385987   \n",
       "2  0.175722   2.283337  0.230247  1.561316  0.677348  0.291276  0.381002   \n",
       "3  0.176463   2.152301  0.207004  1.595086  0.583277  0.296729  0.377087   \n",
       "4  0.173627   2.134014  0.192158  1.504230  0.550960  0.286961  0.363502   \n",
       "\n",
       "     pMEK_N    pNR1_N   pNR2A_N   pNR2B_N  pPKCAB_N    pRSK_N     AKT_N  \\\n",
       "0  0.296927  1.022060  0.605673  1.877684  2.308745  0.441599  0.859366   \n",
       "1  0.281319  0.956676  0.587559  1.725774  2.043037  0.445222  0.834659   \n",
       "2  0.281710  1.003635  0.602449  1.731873  2.017984  0.467668  0.814329   \n",
       "3  0.313832  0.875390  0.520293  1.566852  2.132754  0.477671  0.727705   \n",
       "4  0.277964  0.864912  0.507990  1.480059  2.013697  0.483416  0.687794   \n",
       "\n",
       "     BRAF_N  CAMKII_N    CREB_N     ELK_N     ERK_N   GSK3B_N     JNK_N  \\\n",
       "0  0.416289  0.369608  0.178944  1.866358  3.685247  1.537227  0.264526   \n",
       "1  0.400364  0.356178  0.173680  1.761047  3.485287  1.509249  0.255727   \n",
       "2  0.399847  0.368089  0.173905  1.765544  3.571456  1.501244  0.259614   \n",
       "3  0.385639  0.362970  0.179449  1.286277  2.970137  1.419710  0.259536   \n",
       "4  0.367531  0.355311  0.174836  1.324695  2.896334  1.359876  0.250705   \n",
       "\n",
       "      MEK_N    TRKA_N     RSK_N     APP_N  Bcatenin_N    SOD1_N    MTOR_N  \\\n",
       "0  0.319677  0.813866  0.165846  0.453910    3.037621  0.369510  0.458539   \n",
       "1  0.304419  0.780504  0.157194  0.430940    2.921882  0.342279  0.423560   \n",
       "2  0.311747  0.785154  0.160895  0.423187    2.944136  0.343696  0.425005   \n",
       "3  0.279218  0.734492  0.162210  0.410615    2.500204  0.344509  0.429211   \n",
       "4  0.273667  0.702699  0.154827  0.398550    2.456560  0.329126  0.408755   \n",
       "\n",
       "      P38_N   pMTOR_N   DSCR1_N   AMPKA_N    NR2B_N   pNUMB_N  RAPTOR_N  \\\n",
       "0  0.335336  0.825192  0.576916  0.448099  0.586271  0.394721  0.339571   \n",
       "1  0.324835  0.761718  0.545097  0.420876  0.545097  0.368255  0.321959   \n",
       "2  0.324852  0.757031  0.543620  0.404630  0.552994  0.363880  0.313086   \n",
       "3  0.330121  0.746980  0.546763  0.386860  0.547849  0.366771  0.328492   \n",
       "4  0.313415  0.691956  0.536860  0.360816  0.512824  0.351551  0.312206   \n",
       "\n",
       "    TIAM1_N  pP70S6_N    NUMB_N   P70S6_N  pGSK3B_N   pPKCG_N    CDK5_N  \\\n",
       "0  0.482864  0.294170  0.182150  0.842725  0.192608  1.443091  0.294700   \n",
       "1  0.454519  0.276431  0.182086  0.847615  0.194815  1.439460  0.294060   \n",
       "2  0.447197  0.256648  0.184388  0.856166  0.200737  1.524364  0.301881   \n",
       "3  0.442650  0.398534  0.161768  0.760234  0.184169  1.612382  0.296382   \n",
       "4  0.419095  0.393447  0.160200  0.768113  0.185718  1.645807  0.296829   \n",
       "\n",
       "       S6_N  ADARB1_N  AcetylH3K9_N    RRP1_N     BAX_N     ARC_N   ERBB4_N  \\\n",
       "0  0.354605  1.339070      0.170119  0.159102  0.188852  0.106305  0.144989   \n",
       "1  0.354548  1.306323      0.171427  0.158129  0.184570  0.106592  0.150471   \n",
       "2  0.386087  1.279600      0.185456  0.148696  0.190532  0.108303  0.145330   \n",
       "3  0.290680  1.198765      0.159799  0.166112  0.185323  0.103184  0.140656   \n",
       "4  0.309345  1.206995      0.164650  0.160687  0.188221  0.104784  0.141983   \n",
       "\n",
       "     nNOS_N     Tau_N    GFAP_N   GluR3_N   GluR4_N    IL1B_N   P3525_N  \\\n",
       "0  0.176668  0.125190  0.115291  0.228043  0.142756  0.430957  0.247538   \n",
       "1  0.178309  0.134275  0.118235  0.238073  0.142037  0.457156  0.257632   \n",
       "2  0.176213  0.132560  0.117760  0.244817  0.142445  0.510472  0.255343   \n",
       "3  0.163804  0.123210  0.117439  0.234947  0.145068  0.430996  0.251103   \n",
       "4  0.167710  0.136838  0.116048  0.255528  0.140871  0.481227  0.251773   \n",
       "\n",
       "   pCASP9_N   PSD95_N    SNCA_N  Ubiquitin_N  pGSK3B_Tyr216_N     SHH_N  \\\n",
       "0  1.603310  2.014875  0.108234     1.044979         0.831557  0.188852   \n",
       "1  1.671738  2.004605  0.109749     1.009883         0.849270  0.200404   \n",
       "2  1.663550  2.016831  0.108196     0.996848         0.846709  0.193685   \n",
       "3  1.484624  1.957233  0.119883     0.990225         0.833277  0.192112   \n",
       "4  1.534835  2.009109  0.119524     0.997775         0.878668  0.205604   \n",
       "\n",
       "      BAD_N  BCL2_N     pS6_N   pCFOS_N     SYP_N  H3AcK18_N    EGR1_N  \\\n",
       "0  0.122652     NaN  0.106305  0.108336  0.427099   0.114783  0.131790   \n",
       "1  0.116682     NaN  0.106592  0.104315  0.441581   0.111974  0.135103   \n",
       "2  0.118508     NaN  0.108303  0.106219  0.435777   0.111883  0.133362   \n",
       "3  0.132781     NaN  0.103184  0.111262  0.391691   0.130405  0.147444   \n",
       "4  0.129954     NaN  0.104784  0.110694  0.434154   0.118481  0.140314   \n",
       "\n",
       "   H3MeK4_N    CaNA_N Genotype  Treatment Behavior   class  \n",
       "0  0.128186  1.675652  Control  Memantine      C/S  c-CS-m  \n",
       "1  0.131119  1.743610  Control  Memantine      C/S  c-CS-m  \n",
       "2  0.127431  1.926427  Control  Memantine      C/S  c-CS-m  \n",
       "3  0.146901  1.700563  Control  Memantine      C/S  c-CS-m  \n",
       "4  0.148380  1.839730  Control  Memantine      C/S  c-CS-m  "
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "link = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls'\n",
    "print('URL:', link)\n",
    "\n",
    "df = pd.read_excel(link,\n",
    "                 encoding='utf-8')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1080 entries, 0 to 1079\n",
      "Data columns (total 82 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   MouseID          1080 non-null   object \n",
      " 1   DYRK1A_N         1077 non-null   float64\n",
      " 2   ITSN1_N          1077 non-null   float64\n",
      " 3   BDNF_N           1077 non-null   float64\n",
      " 4   NR1_N            1077 non-null   float64\n",
      " 5   NR2A_N           1077 non-null   float64\n",
      " 6   pAKT_N           1077 non-null   float64\n",
      " 7   pBRAF_N          1077 non-null   float64\n",
      " 8   pCAMKII_N        1077 non-null   float64\n",
      " 9   pCREB_N          1077 non-null   float64\n",
      " 10  pELK_N           1077 non-null   float64\n",
      " 11  pERK_N           1077 non-null   float64\n",
      " 12  pJNK_N           1077 non-null   float64\n",
      " 13  PKCA_N           1077 non-null   float64\n",
      " 14  pMEK_N           1077 non-null   float64\n",
      " 15  pNR1_N           1077 non-null   float64\n",
      " 16  pNR2A_N          1077 non-null   float64\n",
      " 17  pNR2B_N          1077 non-null   float64\n",
      " 18  pPKCAB_N         1077 non-null   float64\n",
      " 19  pRSK_N           1077 non-null   float64\n",
      " 20  AKT_N            1077 non-null   float64\n",
      " 21  BRAF_N           1077 non-null   float64\n",
      " 22  CAMKII_N         1077 non-null   float64\n",
      " 23  CREB_N           1077 non-null   float64\n",
      " 24  ELK_N            1062 non-null   float64\n",
      " 25  ERK_N            1077 non-null   float64\n",
      " 26  GSK3B_N          1077 non-null   float64\n",
      " 27  JNK_N            1077 non-null   float64\n",
      " 28  MEK_N            1073 non-null   float64\n",
      " 29  TRKA_N           1077 non-null   float64\n",
      " 30  RSK_N            1077 non-null   float64\n",
      " 31  APP_N            1077 non-null   float64\n",
      " 32  Bcatenin_N       1062 non-null   float64\n",
      " 33  SOD1_N           1077 non-null   float64\n",
      " 34  MTOR_N           1077 non-null   float64\n",
      " 35  P38_N            1077 non-null   float64\n",
      " 36  pMTOR_N          1077 non-null   float64\n",
      " 37  DSCR1_N          1077 non-null   float64\n",
      " 38  AMPKA_N          1077 non-null   float64\n",
      " 39  NR2B_N           1077 non-null   float64\n",
      " 40  pNUMB_N          1077 non-null   float64\n",
      " 41  RAPTOR_N         1077 non-null   float64\n",
      " 42  TIAM1_N          1077 non-null   float64\n",
      " 43  pP70S6_N         1077 non-null   float64\n",
      " 44  NUMB_N           1080 non-null   float64\n",
      " 45  P70S6_N          1080 non-null   float64\n",
      " 46  pGSK3B_N         1080 non-null   float64\n",
      " 47  pPKCG_N          1080 non-null   float64\n",
      " 48  CDK5_N           1080 non-null   float64\n",
      " 49  S6_N             1080 non-null   float64\n",
      " 50  ADARB1_N         1080 non-null   float64\n",
      " 51  AcetylH3K9_N     1080 non-null   float64\n",
      " 52  RRP1_N           1080 non-null   float64\n",
      " 53  BAX_N            1080 non-null   float64\n",
      " 54  ARC_N            1080 non-null   float64\n",
      " 55  ERBB4_N          1080 non-null   float64\n",
      " 56  nNOS_N           1080 non-null   float64\n",
      " 57  Tau_N            1080 non-null   float64\n",
      " 58  GFAP_N           1080 non-null   float64\n",
      " 59  GluR3_N          1080 non-null   float64\n",
      " 60  GluR4_N          1080 non-null   float64\n",
      " 61  IL1B_N           1080 non-null   float64\n",
      " 62  P3525_N          1080 non-null   float64\n",
      " 63  pCASP9_N         1080 non-null   float64\n",
      " 64  PSD95_N          1080 non-null   float64\n",
      " 65  SNCA_N           1080 non-null   float64\n",
      " 66  Ubiquitin_N      1080 non-null   float64\n",
      " 67  pGSK3B_Tyr216_N  1080 non-null   float64\n",
      " 68  SHH_N            1080 non-null   float64\n",
      " 69  BAD_N            867 non-null    float64\n",
      " 70  BCL2_N           795 non-null    float64\n",
      " 71  pS6_N            1080 non-null   float64\n",
      " 72  pCFOS_N          1005 non-null   float64\n",
      " 73  SYP_N            1080 non-null   float64\n",
      " 74  H3AcK18_N        900 non-null    float64\n",
      " 75  EGR1_N           870 non-null    float64\n",
      " 76  H3MeK4_N         810 non-null    float64\n",
      " 77  CaNA_N           1080 non-null   float64\n",
      " 78  Genotype         1080 non-null   object \n",
      " 79  Treatment        1080 non-null   object \n",
      " 80  Behavior         1080 non-null   object \n",
      " 81  class            1080 non-null   int64  \n",
      "dtypes: float64(77), int64(1), object(4)\n",
      "memory usage: 692.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MouseID              0\n",
       "DYRK1A_N             3\n",
       "ITSN1_N              3\n",
       "BDNF_N               3\n",
       "NR1_N                3\n",
       "NR2A_N               3\n",
       "pAKT_N               3\n",
       "pBRAF_N              3\n",
       "pCAMKII_N            3\n",
       "pCREB_N              3\n",
       "pELK_N               3\n",
       "pERK_N               3\n",
       "pJNK_N               3\n",
       "PKCA_N               3\n",
       "pMEK_N               3\n",
       "pNR1_N               3\n",
       "pNR2A_N              3\n",
       "pNR2B_N              3\n",
       "pPKCAB_N             3\n",
       "pRSK_N               3\n",
       "AKT_N                3\n",
       "BRAF_N               3\n",
       "CAMKII_N             3\n",
       "CREB_N               3\n",
       "ELK_N               18\n",
       "ERK_N                3\n",
       "GSK3B_N              3\n",
       "JNK_N                3\n",
       "MEK_N                7\n",
       "TRKA_N               3\n",
       "RSK_N                3\n",
       "APP_N                3\n",
       "Bcatenin_N          18\n",
       "SOD1_N               3\n",
       "MTOR_N               3\n",
       "P38_N                3\n",
       "pMTOR_N              3\n",
       "DSCR1_N              3\n",
       "AMPKA_N              3\n",
       "NR2B_N               3\n",
       "pNUMB_N              3\n",
       "RAPTOR_N             3\n",
       "TIAM1_N              3\n",
       "pP70S6_N             3\n",
       "NUMB_N               0\n",
       "P70S6_N              0\n",
       "pGSK3B_N             0\n",
       "pPKCG_N              0\n",
       "CDK5_N               0\n",
       "S6_N                 0\n",
       "ADARB1_N             0\n",
       "AcetylH3K9_N         0\n",
       "RRP1_N               0\n",
       "BAX_N                0\n",
       "ARC_N                0\n",
       "ERBB4_N              0\n",
       "nNOS_N               0\n",
       "Tau_N                0\n",
       "GFAP_N               0\n",
       "GluR3_N              0\n",
       "GluR4_N              0\n",
       "IL1B_N               0\n",
       "P3525_N              0\n",
       "pCASP9_N             0\n",
       "PSD95_N              0\n",
       "SNCA_N               0\n",
       "Ubiquitin_N          0\n",
       "pGSK3B_Tyr216_N      0\n",
       "SHH_N                0\n",
       "BAD_N              213\n",
       "BCL2_N             285\n",
       "pS6_N                0\n",
       "pCFOS_N             75\n",
       "SYP_N                0\n",
       "H3AcK18_N          180\n",
       "EGR1_N             210\n",
       "H3MeK4_N           270\n",
       "CaNA_N               0\n",
       "Genotype             0\n",
       "Treatment            0\n",
       "Behavior             0\n",
       "class                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Check missing values\n",
    "#pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##different classes in this dataset\n",
    "\n",
    "#control mice, stimulated to learn, injected with memantine (10 mice)\n",
    "df_cmcs = df.iloc[0:150,:]\n",
    "#control mice, not stimulated to learn, injected with memantine (10 mice)\n",
    "df_cmsc = df.iloc[150:300,:]\n",
    "#control mice, stimulated to learn, injected with saline (9 mice)\n",
    "df_cscs = df.iloc[300:435,:]\n",
    "#control mice, not stimulated to learn, injected with saline (9 mice)\n",
    "df_cssc = df.iloc[435:570,:]\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "#trisomy mice, stimulated to learn, injected with memantine (9 mice)\n",
    "df_tmcs = df.iloc[570:707,:]\n",
    "#trisomy mice, not stimulated to learn, injected with memantine (9 mice)\n",
    "df_tmsc = df.iloc[570:840,:]\n",
    "#trisomy mice, stimulated to learn, injected with saline (7 mice)\n",
    "df_tscs = df.iloc[840:945,:]\n",
    "#trisomy mice, not stimulated to learn, injected with saline (9 mice)\n",
    "df_tssc = df.iloc[945:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##dropped those columns since they were already classified in class labels\n",
    "df1 = df.drop(['MouseID','Genotype', 'Treatment','Behavior'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Replaced missing values with median of that column\n",
    "df2 = df1.fillna(df1.median())\n",
    "# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "# df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapping categorical labels\n",
    "#create a mapping dict to convert class labels from strings to integers\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(df2['class']))}\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DYRK1A_N</th>\n",
       "      <th>ITSN1_N</th>\n",
       "      <th>BDNF_N</th>\n",
       "      <th>NR1_N</th>\n",
       "      <th>NR2A_N</th>\n",
       "      <th>pAKT_N</th>\n",
       "      <th>pBRAF_N</th>\n",
       "      <th>pCAMKII_N</th>\n",
       "      <th>pCREB_N</th>\n",
       "      <th>pELK_N</th>\n",
       "      <th>pERK_N</th>\n",
       "      <th>pJNK_N</th>\n",
       "      <th>PKCA_N</th>\n",
       "      <th>pMEK_N</th>\n",
       "      <th>pNR1_N</th>\n",
       "      <th>pNR2A_N</th>\n",
       "      <th>pNR2B_N</th>\n",
       "      <th>pPKCAB_N</th>\n",
       "      <th>pRSK_N</th>\n",
       "      <th>AKT_N</th>\n",
       "      <th>BRAF_N</th>\n",
       "      <th>CAMKII_N</th>\n",
       "      <th>CREB_N</th>\n",
       "      <th>ELK_N</th>\n",
       "      <th>ERK_N</th>\n",
       "      <th>GSK3B_N</th>\n",
       "      <th>JNK_N</th>\n",
       "      <th>MEK_N</th>\n",
       "      <th>TRKA_N</th>\n",
       "      <th>RSK_N</th>\n",
       "      <th>APP_N</th>\n",
       "      <th>Bcatenin_N</th>\n",
       "      <th>SOD1_N</th>\n",
       "      <th>MTOR_N</th>\n",
       "      <th>P38_N</th>\n",
       "      <th>pMTOR_N</th>\n",
       "      <th>DSCR1_N</th>\n",
       "      <th>AMPKA_N</th>\n",
       "      <th>NR2B_N</th>\n",
       "      <th>pNUMB_N</th>\n",
       "      <th>RAPTOR_N</th>\n",
       "      <th>TIAM1_N</th>\n",
       "      <th>pP70S6_N</th>\n",
       "      <th>NUMB_N</th>\n",
       "      <th>P70S6_N</th>\n",
       "      <th>pGSK3B_N</th>\n",
       "      <th>pPKCG_N</th>\n",
       "      <th>CDK5_N</th>\n",
       "      <th>S6_N</th>\n",
       "      <th>ADARB1_N</th>\n",
       "      <th>AcetylH3K9_N</th>\n",
       "      <th>RRP1_N</th>\n",
       "      <th>BAX_N</th>\n",
       "      <th>ARC_N</th>\n",
       "      <th>ERBB4_N</th>\n",
       "      <th>nNOS_N</th>\n",
       "      <th>Tau_N</th>\n",
       "      <th>GFAP_N</th>\n",
       "      <th>GluR3_N</th>\n",
       "      <th>GluR4_N</th>\n",
       "      <th>IL1B_N</th>\n",
       "      <th>P3525_N</th>\n",
       "      <th>pCASP9_N</th>\n",
       "      <th>PSD95_N</th>\n",
       "      <th>SNCA_N</th>\n",
       "      <th>Ubiquitin_N</th>\n",
       "      <th>pGSK3B_Tyr216_N</th>\n",
       "      <th>SHH_N</th>\n",
       "      <th>BAD_N</th>\n",
       "      <th>BCL2_N</th>\n",
       "      <th>pS6_N</th>\n",
       "      <th>pCFOS_N</th>\n",
       "      <th>SYP_N</th>\n",
       "      <th>H3AcK18_N</th>\n",
       "      <th>EGR1_N</th>\n",
       "      <th>H3MeK4_N</th>\n",
       "      <th>CaNA_N</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.747193</td>\n",
       "      <td>0.430175</td>\n",
       "      <td>2.816329</td>\n",
       "      <td>5.990152</td>\n",
       "      <td>0.218830</td>\n",
       "      <td>0.177565</td>\n",
       "      <td>2.373744</td>\n",
       "      <td>0.232224</td>\n",
       "      <td>1.750936</td>\n",
       "      <td>0.687906</td>\n",
       "      <td>0.306382</td>\n",
       "      <td>0.402698</td>\n",
       "      <td>0.296927</td>\n",
       "      <td>1.022060</td>\n",
       "      <td>0.605673</td>\n",
       "      <td>1.877684</td>\n",
       "      <td>2.308745</td>\n",
       "      <td>0.441599</td>\n",
       "      <td>0.859366</td>\n",
       "      <td>0.416289</td>\n",
       "      <td>0.369608</td>\n",
       "      <td>0.178944</td>\n",
       "      <td>1.866358</td>\n",
       "      <td>3.685247</td>\n",
       "      <td>1.537227</td>\n",
       "      <td>0.264526</td>\n",
       "      <td>0.319677</td>\n",
       "      <td>0.813866</td>\n",
       "      <td>0.165846</td>\n",
       "      <td>0.453910</td>\n",
       "      <td>3.037621</td>\n",
       "      <td>0.369510</td>\n",
       "      <td>0.458539</td>\n",
       "      <td>0.335336</td>\n",
       "      <td>0.825192</td>\n",
       "      <td>0.576916</td>\n",
       "      <td>0.448099</td>\n",
       "      <td>0.586271</td>\n",
       "      <td>0.394721</td>\n",
       "      <td>0.339571</td>\n",
       "      <td>0.482864</td>\n",
       "      <td>0.294170</td>\n",
       "      <td>0.182150</td>\n",
       "      <td>0.842725</td>\n",
       "      <td>0.192608</td>\n",
       "      <td>1.443091</td>\n",
       "      <td>0.294700</td>\n",
       "      <td>0.354605</td>\n",
       "      <td>1.339070</td>\n",
       "      <td>0.170119</td>\n",
       "      <td>0.159102</td>\n",
       "      <td>0.188852</td>\n",
       "      <td>0.106305</td>\n",
       "      <td>0.144989</td>\n",
       "      <td>0.176668</td>\n",
       "      <td>0.125190</td>\n",
       "      <td>0.115291</td>\n",
       "      <td>0.228043</td>\n",
       "      <td>0.142756</td>\n",
       "      <td>0.430957</td>\n",
       "      <td>0.247538</td>\n",
       "      <td>1.603310</td>\n",
       "      <td>2.014875</td>\n",
       "      <td>0.108234</td>\n",
       "      <td>1.044979</td>\n",
       "      <td>0.831557</td>\n",
       "      <td>0.188852</td>\n",
       "      <td>0.122652</td>\n",
       "      <td>0.129468</td>\n",
       "      <td>0.106305</td>\n",
       "      <td>0.108336</td>\n",
       "      <td>0.427099</td>\n",
       "      <td>0.114783</td>\n",
       "      <td>0.131790</td>\n",
       "      <td>0.128186</td>\n",
       "      <td>1.675652</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.514617</td>\n",
       "      <td>0.689064</td>\n",
       "      <td>0.411770</td>\n",
       "      <td>2.789514</td>\n",
       "      <td>5.685038</td>\n",
       "      <td>0.211636</td>\n",
       "      <td>0.172817</td>\n",
       "      <td>2.292150</td>\n",
       "      <td>0.226972</td>\n",
       "      <td>1.596377</td>\n",
       "      <td>0.695006</td>\n",
       "      <td>0.299051</td>\n",
       "      <td>0.385987</td>\n",
       "      <td>0.281319</td>\n",
       "      <td>0.956676</td>\n",
       "      <td>0.587559</td>\n",
       "      <td>1.725774</td>\n",
       "      <td>2.043037</td>\n",
       "      <td>0.445222</td>\n",
       "      <td>0.834659</td>\n",
       "      <td>0.400364</td>\n",
       "      <td>0.356178</td>\n",
       "      <td>0.173680</td>\n",
       "      <td>1.761047</td>\n",
       "      <td>3.485287</td>\n",
       "      <td>1.509249</td>\n",
       "      <td>0.255727</td>\n",
       "      <td>0.304419</td>\n",
       "      <td>0.780504</td>\n",
       "      <td>0.157194</td>\n",
       "      <td>0.430940</td>\n",
       "      <td>2.921882</td>\n",
       "      <td>0.342279</td>\n",
       "      <td>0.423560</td>\n",
       "      <td>0.324835</td>\n",
       "      <td>0.761718</td>\n",
       "      <td>0.545097</td>\n",
       "      <td>0.420876</td>\n",
       "      <td>0.545097</td>\n",
       "      <td>0.368255</td>\n",
       "      <td>0.321959</td>\n",
       "      <td>0.454519</td>\n",
       "      <td>0.276431</td>\n",
       "      <td>0.182086</td>\n",
       "      <td>0.847615</td>\n",
       "      <td>0.194815</td>\n",
       "      <td>1.439460</td>\n",
       "      <td>0.294060</td>\n",
       "      <td>0.354548</td>\n",
       "      <td>1.306323</td>\n",
       "      <td>0.171427</td>\n",
       "      <td>0.158129</td>\n",
       "      <td>0.184570</td>\n",
       "      <td>0.106592</td>\n",
       "      <td>0.150471</td>\n",
       "      <td>0.178309</td>\n",
       "      <td>0.134275</td>\n",
       "      <td>0.118235</td>\n",
       "      <td>0.238073</td>\n",
       "      <td>0.142037</td>\n",
       "      <td>0.457156</td>\n",
       "      <td>0.257632</td>\n",
       "      <td>1.671738</td>\n",
       "      <td>2.004605</td>\n",
       "      <td>0.109749</td>\n",
       "      <td>1.009883</td>\n",
       "      <td>0.849270</td>\n",
       "      <td>0.200404</td>\n",
       "      <td>0.116682</td>\n",
       "      <td>0.129468</td>\n",
       "      <td>0.106592</td>\n",
       "      <td>0.104315</td>\n",
       "      <td>0.441581</td>\n",
       "      <td>0.111974</td>\n",
       "      <td>0.135103</td>\n",
       "      <td>0.131119</td>\n",
       "      <td>1.743610</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.509183</td>\n",
       "      <td>0.730247</td>\n",
       "      <td>0.418309</td>\n",
       "      <td>2.687201</td>\n",
       "      <td>5.622059</td>\n",
       "      <td>0.209011</td>\n",
       "      <td>0.175722</td>\n",
       "      <td>2.283337</td>\n",
       "      <td>0.230247</td>\n",
       "      <td>1.561316</td>\n",
       "      <td>0.677348</td>\n",
       "      <td>0.291276</td>\n",
       "      <td>0.381002</td>\n",
       "      <td>0.281710</td>\n",
       "      <td>1.003635</td>\n",
       "      <td>0.602449</td>\n",
       "      <td>1.731873</td>\n",
       "      <td>2.017984</td>\n",
       "      <td>0.467668</td>\n",
       "      <td>0.814329</td>\n",
       "      <td>0.399847</td>\n",
       "      <td>0.368089</td>\n",
       "      <td>0.173905</td>\n",
       "      <td>1.765544</td>\n",
       "      <td>3.571456</td>\n",
       "      <td>1.501244</td>\n",
       "      <td>0.259614</td>\n",
       "      <td>0.311747</td>\n",
       "      <td>0.785154</td>\n",
       "      <td>0.160895</td>\n",
       "      <td>0.423187</td>\n",
       "      <td>2.944136</td>\n",
       "      <td>0.343696</td>\n",
       "      <td>0.425005</td>\n",
       "      <td>0.324852</td>\n",
       "      <td>0.757031</td>\n",
       "      <td>0.543620</td>\n",
       "      <td>0.404630</td>\n",
       "      <td>0.552994</td>\n",
       "      <td>0.363880</td>\n",
       "      <td>0.313086</td>\n",
       "      <td>0.447197</td>\n",
       "      <td>0.256648</td>\n",
       "      <td>0.184388</td>\n",
       "      <td>0.856166</td>\n",
       "      <td>0.200737</td>\n",
       "      <td>1.524364</td>\n",
       "      <td>0.301881</td>\n",
       "      <td>0.386087</td>\n",
       "      <td>1.279600</td>\n",
       "      <td>0.185456</td>\n",
       "      <td>0.148696</td>\n",
       "      <td>0.190532</td>\n",
       "      <td>0.108303</td>\n",
       "      <td>0.145330</td>\n",
       "      <td>0.176213</td>\n",
       "      <td>0.132560</td>\n",
       "      <td>0.117760</td>\n",
       "      <td>0.244817</td>\n",
       "      <td>0.142445</td>\n",
       "      <td>0.510472</td>\n",
       "      <td>0.255343</td>\n",
       "      <td>1.663550</td>\n",
       "      <td>2.016831</td>\n",
       "      <td>0.108196</td>\n",
       "      <td>0.996848</td>\n",
       "      <td>0.846709</td>\n",
       "      <td>0.193685</td>\n",
       "      <td>0.118508</td>\n",
       "      <td>0.129468</td>\n",
       "      <td>0.108303</td>\n",
       "      <td>0.106219</td>\n",
       "      <td>0.435777</td>\n",
       "      <td>0.111883</td>\n",
       "      <td>0.133362</td>\n",
       "      <td>0.127431</td>\n",
       "      <td>1.926427</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.442107</td>\n",
       "      <td>0.617076</td>\n",
       "      <td>0.358626</td>\n",
       "      <td>2.466947</td>\n",
       "      <td>4.979503</td>\n",
       "      <td>0.222886</td>\n",
       "      <td>0.176463</td>\n",
       "      <td>2.152301</td>\n",
       "      <td>0.207004</td>\n",
       "      <td>1.595086</td>\n",
       "      <td>0.583277</td>\n",
       "      <td>0.296729</td>\n",
       "      <td>0.377087</td>\n",
       "      <td>0.313832</td>\n",
       "      <td>0.875390</td>\n",
       "      <td>0.520293</td>\n",
       "      <td>1.566852</td>\n",
       "      <td>2.132754</td>\n",
       "      <td>0.477671</td>\n",
       "      <td>0.727705</td>\n",
       "      <td>0.385639</td>\n",
       "      <td>0.362970</td>\n",
       "      <td>0.179449</td>\n",
       "      <td>1.286277</td>\n",
       "      <td>2.970137</td>\n",
       "      <td>1.419710</td>\n",
       "      <td>0.259536</td>\n",
       "      <td>0.279218</td>\n",
       "      <td>0.734492</td>\n",
       "      <td>0.162210</td>\n",
       "      <td>0.410615</td>\n",
       "      <td>2.500204</td>\n",
       "      <td>0.344509</td>\n",
       "      <td>0.429211</td>\n",
       "      <td>0.330121</td>\n",
       "      <td>0.746980</td>\n",
       "      <td>0.546763</td>\n",
       "      <td>0.386860</td>\n",
       "      <td>0.547849</td>\n",
       "      <td>0.366771</td>\n",
       "      <td>0.328492</td>\n",
       "      <td>0.442650</td>\n",
       "      <td>0.398534</td>\n",
       "      <td>0.161768</td>\n",
       "      <td>0.760234</td>\n",
       "      <td>0.184169</td>\n",
       "      <td>1.612382</td>\n",
       "      <td>0.296382</td>\n",
       "      <td>0.290680</td>\n",
       "      <td>1.198765</td>\n",
       "      <td>0.159799</td>\n",
       "      <td>0.166112</td>\n",
       "      <td>0.185323</td>\n",
       "      <td>0.103184</td>\n",
       "      <td>0.140656</td>\n",
       "      <td>0.163804</td>\n",
       "      <td>0.123210</td>\n",
       "      <td>0.117439</td>\n",
       "      <td>0.234947</td>\n",
       "      <td>0.145068</td>\n",
       "      <td>0.430996</td>\n",
       "      <td>0.251103</td>\n",
       "      <td>1.484624</td>\n",
       "      <td>1.957233</td>\n",
       "      <td>0.119883</td>\n",
       "      <td>0.990225</td>\n",
       "      <td>0.833277</td>\n",
       "      <td>0.192112</td>\n",
       "      <td>0.132781</td>\n",
       "      <td>0.129468</td>\n",
       "      <td>0.103184</td>\n",
       "      <td>0.111262</td>\n",
       "      <td>0.391691</td>\n",
       "      <td>0.130405</td>\n",
       "      <td>0.147444</td>\n",
       "      <td>0.146901</td>\n",
       "      <td>1.700563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.434940</td>\n",
       "      <td>0.617430</td>\n",
       "      <td>0.358802</td>\n",
       "      <td>2.365785</td>\n",
       "      <td>4.718679</td>\n",
       "      <td>0.213106</td>\n",
       "      <td>0.173627</td>\n",
       "      <td>2.134014</td>\n",
       "      <td>0.192158</td>\n",
       "      <td>1.504230</td>\n",
       "      <td>0.550960</td>\n",
       "      <td>0.286961</td>\n",
       "      <td>0.363502</td>\n",
       "      <td>0.277964</td>\n",
       "      <td>0.864912</td>\n",
       "      <td>0.507990</td>\n",
       "      <td>1.480059</td>\n",
       "      <td>2.013697</td>\n",
       "      <td>0.483416</td>\n",
       "      <td>0.687794</td>\n",
       "      <td>0.367531</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>0.174836</td>\n",
       "      <td>1.324695</td>\n",
       "      <td>2.896334</td>\n",
       "      <td>1.359876</td>\n",
       "      <td>0.250705</td>\n",
       "      <td>0.273667</td>\n",
       "      <td>0.702699</td>\n",
       "      <td>0.154827</td>\n",
       "      <td>0.398550</td>\n",
       "      <td>2.456560</td>\n",
       "      <td>0.329126</td>\n",
       "      <td>0.408755</td>\n",
       "      <td>0.313415</td>\n",
       "      <td>0.691956</td>\n",
       "      <td>0.536860</td>\n",
       "      <td>0.360816</td>\n",
       "      <td>0.512824</td>\n",
       "      <td>0.351551</td>\n",
       "      <td>0.312206</td>\n",
       "      <td>0.419095</td>\n",
       "      <td>0.393447</td>\n",
       "      <td>0.160200</td>\n",
       "      <td>0.768113</td>\n",
       "      <td>0.185718</td>\n",
       "      <td>1.645807</td>\n",
       "      <td>0.296829</td>\n",
       "      <td>0.309345</td>\n",
       "      <td>1.206995</td>\n",
       "      <td>0.164650</td>\n",
       "      <td>0.160687</td>\n",
       "      <td>0.188221</td>\n",
       "      <td>0.104784</td>\n",
       "      <td>0.141983</td>\n",
       "      <td>0.167710</td>\n",
       "      <td>0.136838</td>\n",
       "      <td>0.116048</td>\n",
       "      <td>0.255528</td>\n",
       "      <td>0.140871</td>\n",
       "      <td>0.481227</td>\n",
       "      <td>0.251773</td>\n",
       "      <td>1.534835</td>\n",
       "      <td>2.009109</td>\n",
       "      <td>0.119524</td>\n",
       "      <td>0.997775</td>\n",
       "      <td>0.878668</td>\n",
       "      <td>0.205604</td>\n",
       "      <td>0.129954</td>\n",
       "      <td>0.129468</td>\n",
       "      <td>0.104784</td>\n",
       "      <td>0.110694</td>\n",
       "      <td>0.434154</td>\n",
       "      <td>0.118481</td>\n",
       "      <td>0.140314</td>\n",
       "      <td>0.148380</td>\n",
       "      <td>1.839730</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DYRK1A_N   ITSN1_N    BDNF_N     NR1_N    NR2A_N    pAKT_N   pBRAF_N  \\\n",
       "0  0.503644  0.747193  0.430175  2.816329  5.990152  0.218830  0.177565   \n",
       "1  0.514617  0.689064  0.411770  2.789514  5.685038  0.211636  0.172817   \n",
       "2  0.509183  0.730247  0.418309  2.687201  5.622059  0.209011  0.175722   \n",
       "3  0.442107  0.617076  0.358626  2.466947  4.979503  0.222886  0.176463   \n",
       "4  0.434940  0.617430  0.358802  2.365785  4.718679  0.213106  0.173627   \n",
       "\n",
       "   pCAMKII_N   pCREB_N    pELK_N    pERK_N    pJNK_N    PKCA_N    pMEK_N  \\\n",
       "0   2.373744  0.232224  1.750936  0.687906  0.306382  0.402698  0.296927   \n",
       "1   2.292150  0.226972  1.596377  0.695006  0.299051  0.385987  0.281319   \n",
       "2   2.283337  0.230247  1.561316  0.677348  0.291276  0.381002  0.281710   \n",
       "3   2.152301  0.207004  1.595086  0.583277  0.296729  0.377087  0.313832   \n",
       "4   2.134014  0.192158  1.504230  0.550960  0.286961  0.363502  0.277964   \n",
       "\n",
       "     pNR1_N   pNR2A_N   pNR2B_N  pPKCAB_N    pRSK_N     AKT_N    BRAF_N  \\\n",
       "0  1.022060  0.605673  1.877684  2.308745  0.441599  0.859366  0.416289   \n",
       "1  0.956676  0.587559  1.725774  2.043037  0.445222  0.834659  0.400364   \n",
       "2  1.003635  0.602449  1.731873  2.017984  0.467668  0.814329  0.399847   \n",
       "3  0.875390  0.520293  1.566852  2.132754  0.477671  0.727705  0.385639   \n",
       "4  0.864912  0.507990  1.480059  2.013697  0.483416  0.687794  0.367531   \n",
       "\n",
       "   CAMKII_N    CREB_N     ELK_N     ERK_N   GSK3B_N     JNK_N     MEK_N  \\\n",
       "0  0.369608  0.178944  1.866358  3.685247  1.537227  0.264526  0.319677   \n",
       "1  0.356178  0.173680  1.761047  3.485287  1.509249  0.255727  0.304419   \n",
       "2  0.368089  0.173905  1.765544  3.571456  1.501244  0.259614  0.311747   \n",
       "3  0.362970  0.179449  1.286277  2.970137  1.419710  0.259536  0.279218   \n",
       "4  0.355311  0.174836  1.324695  2.896334  1.359876  0.250705  0.273667   \n",
       "\n",
       "     TRKA_N     RSK_N     APP_N  Bcatenin_N    SOD1_N    MTOR_N     P38_N  \\\n",
       "0  0.813866  0.165846  0.453910    3.037621  0.369510  0.458539  0.335336   \n",
       "1  0.780504  0.157194  0.430940    2.921882  0.342279  0.423560  0.324835   \n",
       "2  0.785154  0.160895  0.423187    2.944136  0.343696  0.425005  0.324852   \n",
       "3  0.734492  0.162210  0.410615    2.500204  0.344509  0.429211  0.330121   \n",
       "4  0.702699  0.154827  0.398550    2.456560  0.329126  0.408755  0.313415   \n",
       "\n",
       "    pMTOR_N   DSCR1_N   AMPKA_N    NR2B_N   pNUMB_N  RAPTOR_N   TIAM1_N  \\\n",
       "0  0.825192  0.576916  0.448099  0.586271  0.394721  0.339571  0.482864   \n",
       "1  0.761718  0.545097  0.420876  0.545097  0.368255  0.321959  0.454519   \n",
       "2  0.757031  0.543620  0.404630  0.552994  0.363880  0.313086  0.447197   \n",
       "3  0.746980  0.546763  0.386860  0.547849  0.366771  0.328492  0.442650   \n",
       "4  0.691956  0.536860  0.360816  0.512824  0.351551  0.312206  0.419095   \n",
       "\n",
       "   pP70S6_N    NUMB_N   P70S6_N  pGSK3B_N   pPKCG_N    CDK5_N      S6_N  \\\n",
       "0  0.294170  0.182150  0.842725  0.192608  1.443091  0.294700  0.354605   \n",
       "1  0.276431  0.182086  0.847615  0.194815  1.439460  0.294060  0.354548   \n",
       "2  0.256648  0.184388  0.856166  0.200737  1.524364  0.301881  0.386087   \n",
       "3  0.398534  0.161768  0.760234  0.184169  1.612382  0.296382  0.290680   \n",
       "4  0.393447  0.160200  0.768113  0.185718  1.645807  0.296829  0.309345   \n",
       "\n",
       "   ADARB1_N  AcetylH3K9_N    RRP1_N     BAX_N     ARC_N   ERBB4_N    nNOS_N  \\\n",
       "0  1.339070      0.170119  0.159102  0.188852  0.106305  0.144989  0.176668   \n",
       "1  1.306323      0.171427  0.158129  0.184570  0.106592  0.150471  0.178309   \n",
       "2  1.279600      0.185456  0.148696  0.190532  0.108303  0.145330  0.176213   \n",
       "3  1.198765      0.159799  0.166112  0.185323  0.103184  0.140656  0.163804   \n",
       "4  1.206995      0.164650  0.160687  0.188221  0.104784  0.141983  0.167710   \n",
       "\n",
       "      Tau_N    GFAP_N   GluR3_N   GluR4_N    IL1B_N   P3525_N  pCASP9_N  \\\n",
       "0  0.125190  0.115291  0.228043  0.142756  0.430957  0.247538  1.603310   \n",
       "1  0.134275  0.118235  0.238073  0.142037  0.457156  0.257632  1.671738   \n",
       "2  0.132560  0.117760  0.244817  0.142445  0.510472  0.255343  1.663550   \n",
       "3  0.123210  0.117439  0.234947  0.145068  0.430996  0.251103  1.484624   \n",
       "4  0.136838  0.116048  0.255528  0.140871  0.481227  0.251773  1.534835   \n",
       "\n",
       "    PSD95_N    SNCA_N  Ubiquitin_N  pGSK3B_Tyr216_N     SHH_N     BAD_N  \\\n",
       "0  2.014875  0.108234     1.044979         0.831557  0.188852  0.122652   \n",
       "1  2.004605  0.109749     1.009883         0.849270  0.200404  0.116682   \n",
       "2  2.016831  0.108196     0.996848         0.846709  0.193685  0.118508   \n",
       "3  1.957233  0.119883     0.990225         0.833277  0.192112  0.132781   \n",
       "4  2.009109  0.119524     0.997775         0.878668  0.205604  0.129954   \n",
       "\n",
       "     BCL2_N     pS6_N   pCFOS_N     SYP_N  H3AcK18_N    EGR1_N  H3MeK4_N  \\\n",
       "0  0.129468  0.106305  0.108336  0.427099   0.114783  0.131790  0.128186   \n",
       "1  0.129468  0.106592  0.104315  0.441581   0.111974  0.135103  0.131119   \n",
       "2  0.129468  0.108303  0.106219  0.435777   0.111883  0.133362  0.127431   \n",
       "3  0.129468  0.103184  0.111262  0.391691   0.130405  0.147444  0.146901   \n",
       "4  0.129468  0.104784  0.110694  0.434154   0.118481  0.140314  0.148380   \n",
       "\n",
       "     CaNA_N  class  \n",
       "0  1.675652      0  \n",
       "1  1.743610      0  \n",
       "2  1.926427      0  \n",
       "3  1.700563      0  \n",
       "4  1.839730      0  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['class'] = df2['class'].map(class_mapping)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels: [0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "X = df2.iloc[:,0:77]\n",
    "y = df2['class']\n",
    "print('Class labels:', np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels counts in y: [150 135 150 135 135 105 135 135]\n",
      "Labels counts in y_train: [105  94 105  95  95  73  95  94]\n",
      "Labels counts in y_test: [45 41 45 40 40 32 40 41]\n"
     ]
    }
   ],
   "source": [
    "print('Labels counts in y:', np.bincount(y))\n",
    "print('Labels counts in y_train:', np.bincount(y_train))\n",
    "print('Labels counts in y_test:', np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train)\n",
    "X_test_std = stdsc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02464966 0.02480992 0.00540723 0.00696427 0.0062427  0.00866212\n",
      " 0.00503656 0.02291218 0.00795464 0.00776085 0.03279713 0.0086505\n",
      " 0.01088154 0.00717254 0.00616276 0.01250943 0.00480879 0.02123562\n",
      " 0.00887318 0.01724414 0.02347233 0.0104742  0.00516748 0.00716803\n",
      " 0.00646515 0.00719325 0.0059832  0.00465343 0.00770181 0.0049547\n",
      " 0.03070428 0.00461055 0.05690708 0.01085409 0.01943347 0.01260835\n",
      " 0.007825   0.00790622 0.01052922 0.02254109 0.0091851  0.00882635\n",
      " 0.02175059 0.00887811 0.00609111 0.01982064 0.03677888 0.00844378\n",
      " 0.01909633 0.01222847 0.01787664 0.01019886 0.00724461 0.023611\n",
      " 0.01289316 0.01298205 0.02346267 0.008416   0.00742221 0.00622793\n",
      " 0.01291756 0.00989401 0.00782419 0.00866483 0.01049812 0.03155812\n",
      " 0.00943907 0.00769528 0.00679757 0.00549169 0.02227785 0.00446693\n",
      " 0.00772792 0.01232522 0.00417437 0.00639394 0.03253015]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "feat_labels = df1.columns[:77]\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=500,\n",
    "                                random_state=1)\n",
    "\n",
    "forest.fit(X_train_std, y_train)\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(X_train_std.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30, \n",
    "                            feat_labels[indices[f]], \n",
    "                            importances[indices[f]]))\n",
    "\n",
    "plt.title('Feature Importance')\n",
    "plt.bar(range(X_train_std.shape[1]), \n",
    "        importances[indices],\n",
    "        align='center')\n",
    "\n",
    "plt.xticks(range(X_train.shape[1]), \n",
    "           feat_labels[indices], rotation=90)\n",
    "plt.xlim([-1, X_train_std.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "##Proteins that had missing values \n",
    "# BAD_N              213\n",
    "# BCL2_N             285\n",
    "# pCFOS_N             75\n",
    "# H3AcK18_N          180\n",
    "# EGR1_N             210\n",
    "# H3MeK4_N           270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DYRK1A_N</th>\n",
       "      <th>ITSN1_N</th>\n",
       "      <th>BDNF_N</th>\n",
       "      <th>NR1_N</th>\n",
       "      <th>NR2A_N</th>\n",
       "      <th>pAKT_N</th>\n",
       "      <th>pBRAF_N</th>\n",
       "      <th>pCAMKII_N</th>\n",
       "      <th>pCREB_N</th>\n",
       "      <th>pELK_N</th>\n",
       "      <th>...</th>\n",
       "      <th>pCASP9_N</th>\n",
       "      <th>PSD95_N</th>\n",
       "      <th>SNCA_N</th>\n",
       "      <th>Ubiquitin_N</th>\n",
       "      <th>pGSK3B_Tyr216_N</th>\n",
       "      <th>SHH_N</th>\n",
       "      <th>pS6_N</th>\n",
       "      <th>SYP_N</th>\n",
       "      <th>CaNA_N</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.747193</td>\n",
       "      <td>0.430175</td>\n",
       "      <td>2.816329</td>\n",
       "      <td>5.990152</td>\n",
       "      <td>0.218830</td>\n",
       "      <td>0.177565</td>\n",
       "      <td>2.373744</td>\n",
       "      <td>0.232224</td>\n",
       "      <td>1.750936</td>\n",
       "      <td>...</td>\n",
       "      <td>1.603310</td>\n",
       "      <td>2.014875</td>\n",
       "      <td>0.108234</td>\n",
       "      <td>1.044979</td>\n",
       "      <td>0.831557</td>\n",
       "      <td>0.188852</td>\n",
       "      <td>0.106305</td>\n",
       "      <td>0.427099</td>\n",
       "      <td>1.675652</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.514617</td>\n",
       "      <td>0.689064</td>\n",
       "      <td>0.411770</td>\n",
       "      <td>2.789514</td>\n",
       "      <td>5.685038</td>\n",
       "      <td>0.211636</td>\n",
       "      <td>0.172817</td>\n",
       "      <td>2.292150</td>\n",
       "      <td>0.226972</td>\n",
       "      <td>1.596377</td>\n",
       "      <td>...</td>\n",
       "      <td>1.671738</td>\n",
       "      <td>2.004605</td>\n",
       "      <td>0.109749</td>\n",
       "      <td>1.009883</td>\n",
       "      <td>0.849270</td>\n",
       "      <td>0.200404</td>\n",
       "      <td>0.106592</td>\n",
       "      <td>0.441581</td>\n",
       "      <td>1.743610</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.509183</td>\n",
       "      <td>0.730247</td>\n",
       "      <td>0.418309</td>\n",
       "      <td>2.687201</td>\n",
       "      <td>5.622059</td>\n",
       "      <td>0.209011</td>\n",
       "      <td>0.175722</td>\n",
       "      <td>2.283337</td>\n",
       "      <td>0.230247</td>\n",
       "      <td>1.561316</td>\n",
       "      <td>...</td>\n",
       "      <td>1.663550</td>\n",
       "      <td>2.016831</td>\n",
       "      <td>0.108196</td>\n",
       "      <td>0.996848</td>\n",
       "      <td>0.846709</td>\n",
       "      <td>0.193685</td>\n",
       "      <td>0.108303</td>\n",
       "      <td>0.435777</td>\n",
       "      <td>1.926427</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.442107</td>\n",
       "      <td>0.617076</td>\n",
       "      <td>0.358626</td>\n",
       "      <td>2.466947</td>\n",
       "      <td>4.979503</td>\n",
       "      <td>0.222886</td>\n",
       "      <td>0.176463</td>\n",
       "      <td>2.152301</td>\n",
       "      <td>0.207004</td>\n",
       "      <td>1.595086</td>\n",
       "      <td>...</td>\n",
       "      <td>1.484624</td>\n",
       "      <td>1.957233</td>\n",
       "      <td>0.119883</td>\n",
       "      <td>0.990225</td>\n",
       "      <td>0.833277</td>\n",
       "      <td>0.192112</td>\n",
       "      <td>0.103184</td>\n",
       "      <td>0.391691</td>\n",
       "      <td>1.700563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.434940</td>\n",
       "      <td>0.617430</td>\n",
       "      <td>0.358802</td>\n",
       "      <td>2.365785</td>\n",
       "      <td>4.718679</td>\n",
       "      <td>0.213106</td>\n",
       "      <td>0.173627</td>\n",
       "      <td>2.134014</td>\n",
       "      <td>0.192158</td>\n",
       "      <td>1.504230</td>\n",
       "      <td>...</td>\n",
       "      <td>1.534835</td>\n",
       "      <td>2.009109</td>\n",
       "      <td>0.119524</td>\n",
       "      <td>0.997775</td>\n",
       "      <td>0.878668</td>\n",
       "      <td>0.205604</td>\n",
       "      <td>0.104784</td>\n",
       "      <td>0.434154</td>\n",
       "      <td>1.839730</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>0.254860</td>\n",
       "      <td>0.463591</td>\n",
       "      <td>0.254860</td>\n",
       "      <td>2.092082</td>\n",
       "      <td>2.600035</td>\n",
       "      <td>0.211736</td>\n",
       "      <td>0.171262</td>\n",
       "      <td>2.483740</td>\n",
       "      <td>0.207317</td>\n",
       "      <td>1.057971</td>\n",
       "      <td>...</td>\n",
       "      <td>1.323554</td>\n",
       "      <td>2.578046</td>\n",
       "      <td>0.167181</td>\n",
       "      <td>1.261651</td>\n",
       "      <td>0.962942</td>\n",
       "      <td>0.275547</td>\n",
       "      <td>0.115806</td>\n",
       "      <td>0.374088</td>\n",
       "      <td>1.364823</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>0.272198</td>\n",
       "      <td>0.474163</td>\n",
       "      <td>0.251638</td>\n",
       "      <td>2.161390</td>\n",
       "      <td>2.801492</td>\n",
       "      <td>0.251274</td>\n",
       "      <td>0.182496</td>\n",
       "      <td>2.512737</td>\n",
       "      <td>0.216339</td>\n",
       "      <td>1.081150</td>\n",
       "      <td>...</td>\n",
       "      <td>1.275605</td>\n",
       "      <td>2.534347</td>\n",
       "      <td>0.169592</td>\n",
       "      <td>1.254872</td>\n",
       "      <td>0.983690</td>\n",
       "      <td>0.283207</td>\n",
       "      <td>0.113614</td>\n",
       "      <td>0.375259</td>\n",
       "      <td>1.364478</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>0.228700</td>\n",
       "      <td>0.395179</td>\n",
       "      <td>0.234118</td>\n",
       "      <td>1.733184</td>\n",
       "      <td>2.220852</td>\n",
       "      <td>0.220665</td>\n",
       "      <td>0.161435</td>\n",
       "      <td>1.989723</td>\n",
       "      <td>0.185164</td>\n",
       "      <td>0.884342</td>\n",
       "      <td>...</td>\n",
       "      <td>1.437534</td>\n",
       "      <td>2.544515</td>\n",
       "      <td>0.179692</td>\n",
       "      <td>1.242248</td>\n",
       "      <td>0.976609</td>\n",
       "      <td>0.290843</td>\n",
       "      <td>0.118948</td>\n",
       "      <td>0.422121</td>\n",
       "      <td>1.430825</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>0.221242</td>\n",
       "      <td>0.412894</td>\n",
       "      <td>0.243974</td>\n",
       "      <td>1.876347</td>\n",
       "      <td>2.384088</td>\n",
       "      <td>0.208897</td>\n",
       "      <td>0.173623</td>\n",
       "      <td>2.086028</td>\n",
       "      <td>0.192044</td>\n",
       "      <td>0.922595</td>\n",
       "      <td>...</td>\n",
       "      <td>1.498820</td>\n",
       "      <td>2.609769</td>\n",
       "      <td>0.185037</td>\n",
       "      <td>1.301071</td>\n",
       "      <td>0.989286</td>\n",
       "      <td>0.306701</td>\n",
       "      <td>0.125295</td>\n",
       "      <td>0.397676</td>\n",
       "      <td>1.404031</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>0.302626</td>\n",
       "      <td>0.461059</td>\n",
       "      <td>0.256564</td>\n",
       "      <td>2.092790</td>\n",
       "      <td>2.594348</td>\n",
       "      <td>0.251001</td>\n",
       "      <td>0.191811</td>\n",
       "      <td>2.361816</td>\n",
       "      <td>0.223632</td>\n",
       "      <td>1.064085</td>\n",
       "      <td>...</td>\n",
       "      <td>1.490077</td>\n",
       "      <td>2.526372</td>\n",
       "      <td>0.184516</td>\n",
       "      <td>1.267120</td>\n",
       "      <td>1.020383</td>\n",
       "      <td>0.292330</td>\n",
       "      <td>0.118899</td>\n",
       "      <td>0.420347</td>\n",
       "      <td>1.370999</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1080 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DYRK1A_N   ITSN1_N    BDNF_N     NR1_N    NR2A_N    pAKT_N   pBRAF_N  \\\n",
       "0     0.503644  0.747193  0.430175  2.816329  5.990152  0.218830  0.177565   \n",
       "1     0.514617  0.689064  0.411770  2.789514  5.685038  0.211636  0.172817   \n",
       "2     0.509183  0.730247  0.418309  2.687201  5.622059  0.209011  0.175722   \n",
       "3     0.442107  0.617076  0.358626  2.466947  4.979503  0.222886  0.176463   \n",
       "4     0.434940  0.617430  0.358802  2.365785  4.718679  0.213106  0.173627   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1075  0.254860  0.463591  0.254860  2.092082  2.600035  0.211736  0.171262   \n",
       "1076  0.272198  0.474163  0.251638  2.161390  2.801492  0.251274  0.182496   \n",
       "1077  0.228700  0.395179  0.234118  1.733184  2.220852  0.220665  0.161435   \n",
       "1078  0.221242  0.412894  0.243974  1.876347  2.384088  0.208897  0.173623   \n",
       "1079  0.302626  0.461059  0.256564  2.092790  2.594348  0.251001  0.191811   \n",
       "\n",
       "      pCAMKII_N   pCREB_N    pELK_N  ...  pCASP9_N   PSD95_N    SNCA_N  \\\n",
       "0      2.373744  0.232224  1.750936  ...  1.603310  2.014875  0.108234   \n",
       "1      2.292150  0.226972  1.596377  ...  1.671738  2.004605  0.109749   \n",
       "2      2.283337  0.230247  1.561316  ...  1.663550  2.016831  0.108196   \n",
       "3      2.152301  0.207004  1.595086  ...  1.484624  1.957233  0.119883   \n",
       "4      2.134014  0.192158  1.504230  ...  1.534835  2.009109  0.119524   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "1075   2.483740  0.207317  1.057971  ...  1.323554  2.578046  0.167181   \n",
       "1076   2.512737  0.216339  1.081150  ...  1.275605  2.534347  0.169592   \n",
       "1077   1.989723  0.185164  0.884342  ...  1.437534  2.544515  0.179692   \n",
       "1078   2.086028  0.192044  0.922595  ...  1.498820  2.609769  0.185037   \n",
       "1079   2.361816  0.223632  1.064085  ...  1.490077  2.526372  0.184516   \n",
       "\n",
       "      Ubiquitin_N  pGSK3B_Tyr216_N     SHH_N     pS6_N     SYP_N    CaNA_N  \\\n",
       "0        1.044979         0.831557  0.188852  0.106305  0.427099  1.675652   \n",
       "1        1.009883         0.849270  0.200404  0.106592  0.441581  1.743610   \n",
       "2        0.996848         0.846709  0.193685  0.108303  0.435777  1.926427   \n",
       "3        0.990225         0.833277  0.192112  0.103184  0.391691  1.700563   \n",
       "4        0.997775         0.878668  0.205604  0.104784  0.434154  1.839730   \n",
       "...           ...              ...       ...       ...       ...       ...   \n",
       "1075     1.261651         0.962942  0.275547  0.115806  0.374088  1.364823   \n",
       "1076     1.254872         0.983690  0.283207  0.113614  0.375259  1.364478   \n",
       "1077     1.242248         0.976609  0.290843  0.118948  0.422121  1.430825   \n",
       "1078     1.301071         0.989286  0.306701  0.125295  0.397676  1.404031   \n",
       "1079     1.267120         1.020383  0.292330  0.118899  0.420347  1.370999   \n",
       "\n",
       "      class  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "1075      7  \n",
       "1076      7  \n",
       "1077      7  \n",
       "1078      7  \n",
       "1079      7  \n",
       "\n",
       "[1080 rows x 72 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##remove columns that had missing values\n",
    "df_new = df2.drop(['BAD_N','BCL2_N','pCFOS_N','H3AcK18_N','EGR1_N','H3MeK4_N'],axis=1)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df_new.iloc[:,0:71]\n",
    "y1 = df_new['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "    X1, y1, test_size=0.3, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdsc = StandardScaler()\n",
    "X_train_std1 = stdsc.fit_transform(X_train1)\n",
    "X_test_std1 = stdsc.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) SOD1_N                         0.056866\n",
      " 2) pPKCG_N                        0.041463\n",
      " 3) pERK_N                         0.039936\n",
      " 4) APP_N                          0.034388\n",
      " 5) CaNA_N                         0.031758\n",
      " 6) Ubiquitin_N                    0.031019\n",
      " 7) DYRK1A_N                       0.028189\n",
      " 8) pS6_N                          0.027400\n",
      " 9) ITSN1_N                        0.027388\n",
      "10) pCAMKII_N                      0.025972\n",
      "11) BRAF_N                         0.025641\n",
      "12) ARC_N                          0.024948\n",
      "13) pP70S6_N                       0.022392\n",
      "14) Tau_N                          0.022034\n",
      "15) P38_N                          0.021647\n",
      "16) pPKCAB_N                       0.021611\n",
      "17) pNUMB_N                        0.020186\n",
      "18) pGSK3B_N                       0.018748\n",
      "19) S6_N                           0.018389\n",
      "20) AcetylH3K9_N                   0.017300\n",
      "21) AKT_N                          0.016902\n",
      "22) pMTOR_N                        0.015397\n",
      "23) MTOR_N                         0.014425\n",
      "24) nNOS_N                         0.014208\n",
      "25) ADARB1_N                       0.013215\n",
      "26) pNR2A_N                        0.013185\n",
      "27) IL1B_N                         0.012446\n",
      "28) PKCA_N                         0.012213\n",
      "29) RAPTOR_N                       0.011471\n",
      "30) pJNK_N                         0.010722\n",
      "31) ERBB4_N                        0.010637\n",
      "32) NR2B_N                         0.010619\n",
      "33) NUMB_N                         0.010266\n",
      "34) RRP1_N                         0.009724\n",
      "35) SNCA_N                         0.009443\n",
      "36) pGSK3B_Tyr216_N                0.009380\n",
      "37) TIAM1_N                        0.009375\n",
      "38) SYP_N                          0.009273\n",
      "39) pRSK_N                         0.009126\n",
      "40) AMPKA_N                        0.009040\n",
      "41) SHH_N                          0.009033\n",
      "42) P3525_N                        0.008896\n",
      "43) PSD95_N                        0.008881\n",
      "44) DSCR1_N                        0.008809\n",
      "45) GFAP_N                         0.008282\n",
      "46) pAKT_N                         0.008014\n",
      "47) CDK5_N                         0.007987\n",
      "48) GluR3_N                        0.007985\n",
      "49) pCASP9_N                       0.007961\n",
      "50) pELK_N                         0.007848\n",
      "51) CAMKII_N                       0.007640\n",
      "52) pMEK_N                         0.007609\n",
      "53) BAX_N                          0.007369\n",
      "54) GSK3B_N                        0.007330\n",
      "55) TRKA_N                         0.007263\n",
      "56) pCREB_N                        0.006954\n",
      "57) GluR4_N                        0.006676\n",
      "58) NR1_N                          0.006547\n",
      "59) JNK_N                          0.006045\n",
      "60) BDNF_N                         0.005991\n",
      "61) ERK_N                          0.005830\n",
      "62) pBRAF_N                        0.005817\n",
      "63) pNR1_N                         0.005755\n",
      "64) P70S6_N                        0.005718\n",
      "65) ELK_N                          0.005700\n",
      "66) NR2A_N                         0.005446\n",
      "67) CREB_N                         0.005402\n",
      "68) pNR2B_N                        0.005209\n",
      "69) RSK_N                          0.004828\n",
      "70) Bcatenin_N                     0.004487\n",
      "71) MEK_N                          0.004345\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "feat_labels = df_new.columns[:71]\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=500,\n",
    "                                random_state=1)\n",
    "\n",
    "forest.fit(X_train_std1, y_train1)\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(X_train_std1.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30, \n",
    "                            feat_labels[indices[f]], \n",
    "                            importances[indices[f]]))\n",
    "\n",
    "plt.title('Feature Importance')\n",
    "plt.bar(range(X_train_std1.shape[1]), \n",
    "        importances[indices],\n",
    "        align='center')\n",
    "\n",
    "plt.xticks(range(X_train_std1.shape[1]), \n",
    "           feat_labels[indices], rotation=90)\n",
    "plt.xlim([-1, X_train_std1.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features that meet this threshold criterion: 6\n"
     ]
    }
   ],
   "source": [
    "##Subset of features\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "sfm = SelectFromModel(forest, threshold=0.03, prefit=True)\n",
    "X_selected = sfm.transform(X_train_std1)\n",
    "print('Number of features that meet this threshold criterion:', \n",
    "      X_selected.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) SOD1_N                         0.056866\n",
      " 2) pPKCG_N                        0.041463\n",
      " 3) pERK_N                         0.039936\n",
      " 4) APP_N                          0.034388\n",
      " 5) CaNA_N                         0.031758\n",
      " 6) Ubiquitin_N                    0.031019\n"
     ]
    }
   ],
   "source": [
    "for f in range(X_selected.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30, \n",
    "                            feat_labels[indices[f]], \n",
    "                            importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOD1_N</th>\n",
       "      <th>pPKCG_N</th>\n",
       "      <th>pERK_N</th>\n",
       "      <th>APP_N</th>\n",
       "      <th>CaNA_N</th>\n",
       "      <th>Ubiquitin_N</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.369510</td>\n",
       "      <td>1.443091</td>\n",
       "      <td>0.687906</td>\n",
       "      <td>0.453910</td>\n",
       "      <td>1.675652</td>\n",
       "      <td>1.044979</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.342279</td>\n",
       "      <td>1.439460</td>\n",
       "      <td>0.695006</td>\n",
       "      <td>0.430940</td>\n",
       "      <td>1.743610</td>\n",
       "      <td>1.009883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.343696</td>\n",
       "      <td>1.524364</td>\n",
       "      <td>0.677348</td>\n",
       "      <td>0.423187</td>\n",
       "      <td>1.926427</td>\n",
       "      <td>0.996848</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.344509</td>\n",
       "      <td>1.612382</td>\n",
       "      <td>0.583277</td>\n",
       "      <td>0.410615</td>\n",
       "      <td>1.700563</td>\n",
       "      <td>0.990225</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.329126</td>\n",
       "      <td>1.645807</td>\n",
       "      <td>0.550960</td>\n",
       "      <td>0.398550</td>\n",
       "      <td>1.839730</td>\n",
       "      <td>0.997775</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>0.820078</td>\n",
       "      <td>2.630825</td>\n",
       "      <td>0.265642</td>\n",
       "      <td>0.372216</td>\n",
       "      <td>1.364823</td>\n",
       "      <td>1.261651</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>0.854258</td>\n",
       "      <td>2.593227</td>\n",
       "      <td>0.270378</td>\n",
       "      <td>0.360990</td>\n",
       "      <td>1.364478</td>\n",
       "      <td>1.254872</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>0.661809</td>\n",
       "      <td>2.628286</td>\n",
       "      <td>0.255045</td>\n",
       "      <td>0.309978</td>\n",
       "      <td>1.430825</td>\n",
       "      <td>1.242248</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>0.698413</td>\n",
       "      <td>2.659706</td>\n",
       "      <td>0.230649</td>\n",
       "      <td>0.341172</td>\n",
       "      <td>1.404031</td>\n",
       "      <td>1.301071</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>0.786827</td>\n",
       "      <td>2.654926</td>\n",
       "      <td>0.276146</td>\n",
       "      <td>0.402982</td>\n",
       "      <td>1.370999</td>\n",
       "      <td>1.267120</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1080 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SOD1_N   pPKCG_N    pERK_N     APP_N    CaNA_N  Ubiquitin_N  class\n",
       "0     0.369510  1.443091  0.687906  0.453910  1.675652     1.044979      0\n",
       "1     0.342279  1.439460  0.695006  0.430940  1.743610     1.009883      0\n",
       "2     0.343696  1.524364  0.677348  0.423187  1.926427     0.996848      0\n",
       "3     0.344509  1.612382  0.583277  0.410615  1.700563     0.990225      0\n",
       "4     0.329126  1.645807  0.550960  0.398550  1.839730     0.997775      0\n",
       "...        ...       ...       ...       ...       ...          ...    ...\n",
       "1075  0.820078  2.630825  0.265642  0.372216  1.364823     1.261651      7\n",
       "1076  0.854258  2.593227  0.270378  0.360990  1.364478     1.254872      7\n",
       "1077  0.661809  2.628286  0.255045  0.309978  1.430825     1.242248      7\n",
       "1078  0.698413  2.659706  0.230649  0.341172  1.404031     1.301071      7\n",
       "1079  0.786827  2.654926  0.276146  0.402982  1.370999     1.267120      7\n",
       "\n",
       "[1080 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##construct a subset of the data\n",
    "\n",
    "c1 = df_new[['class']]\n",
    "c2 = df_new[['SOD1_N','pPKCG_N','pERK_N','APP_N','CaNA_N','Ubiquitin_N']]\n",
    "\n",
    "data2 = pd.concat([c2,c1], axis=1)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Similarily, split the selected feature dataset into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_sub, y_sub = data2.iloc[:, 0:6].values, data2.iloc[:, -1].values\n",
    "\n",
    "X_train_sub, X_test_sub, y_train_sub, y_test_sub =\\\n",
    "    train_test_split(X_sub, y_sub, \n",
    "                     test_size=0.3, \n",
    "                     random_state=0, \n",
    "                     stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdsc = StandardScaler()\n",
    "X_train_sub_std = stdsc.fit_transform(X_train_sub)\n",
    "X_test_sub_std = stdsc.transform(X_test_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='multinomial', random_state=1, solver='sag')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=1, solver='sag', multi_class='multinomial')\n",
    "lr.fit(X_train_sub_std, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7142857142857143\n",
      "Test accuracy: 0.6975308641975309\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy:', lr.score(X_train_sub_std, y_train_sub))\n",
    "print('Test accuracy:', lr.score(X_test_sub_std, y_test_sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression didn't perform well on the given dataset where in there is only 68% accuracy of correctly evaluating the group's classification. The reason could have been due to the presence of many classes that are difficult to segregate into its own groups in a logistic regression model. Perhaphs it would have done better in three to four classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=50, n_jobs=2, random_state=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "##modify the number of estimators to be larger, and then smaller, than the current value\n",
    "forest = RandomForestClassifier(criterion='gini',\n",
    "                                n_estimators=50, \n",
    "                                random_state=1,\n",
    "                                n_jobs=2)\n",
    "forest.fit(X_train_sub_std, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy:', forest.score(X_train_sub_std, y_train_sub))\n",
    "print('Test accuracy:', forest.score(X_test_sub_std, y_test_sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the accuracy_scores in Random Forest, we can see that theres a 100% accuraty rate on training dataset. This maybe due to overfitting which has not evaluated testing data exactly as it should to training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8412698412698413\n",
      "Test accuracy: 0.7993827160493827\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=15)\n",
    "knn.fit(X_train_sub_std, y_train_sub)\n",
    "print('Training accuracy:', knn.score(X_train_sub_std, y_train_sub))\n",
    "print('Test accuracy:', knn.score(X_test_sub_std, y_test_sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knn classification does fairly better than the above two algorithms. Although there is a slight difference in testing and training dataset, which could be a resultant of few misclassification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Experiment Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was collected from an API - UCI Machine Learning website, which was originally collected from an archive. Data preprocessing involved first with the removal of columns MouseID, Genotype, Treatment and Behavior. The last column is the class label which basically summarises genotype,treatment and behavious for each class. The next step was to address the missing values. There were no recordings of certain protein in some mice. However, the missing values were filled with median values at the initial step. In the next process, class labels were converted to integers by mapping. The features and target set were shuffled before spliting into training and test (70:30 ratio). Further, the data was standardized using StandardScalar feature for evaluating classification using different models. In this dataset, Random Forest Classifier was used to select important features which differentiate amongst the classes. Some of the important proteins are - SOD1_N, pPKCG_N, pERK_N, CaNA_N and APP_N. Refering back to missing values, there were some missing protein measurements BAD_N, BCL2_N, pCFOS_N, H3AcK18_N, EGR1_N, H3MeK4_N. Since none of these proteins were important in feature selection process, these proteins were removed from the original dataset. Feature selection was attempted again after removal of those columns. The columns with most important features were SOD1_N, pPKCG_N, pERK_N, APP_N, CaNA_N, Ubiquitin_N. Class imbalnce was not significant amongst all the classes in this dataset.\n",
    "Three models were used to evaluate classification - Multiclass logistic Regression, Random Forest and KNN classification. From all the three models KNN classification performed the best. Althought, Random Forest was the best model, it showed an accuracy of 100% on the training data which may have lead to overfitting. I think the models evaluated accurately as I hoped for since there were multiple classes to address to. Biological data is often intricate to classify subtle differences. Nevertheless its always essential to address these differences to understand underlying effects on the surface. This dataset may have some overlapping outcome but genetic or protein level differences are always interesting to understand the interplay of pathways that ultimately result into one outcome. According to me, feature extraction is always challenging in biological data since its difficult to classify subtle differences. Its difficult to set the criteria of selecting features. Instead of observing for global expression of proteins in all the classes, it would have been more informative if this study was done by comparing fewer classes to answer specific learning disability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
